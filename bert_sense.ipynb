{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sense.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9-PUGvpjOZ",
        "colab_type": "code",
        "outputId": "af579e48-e0a1-40c2-a1d2-62dbb72ab714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "! pip install bs4\n",
        "! pip install contractions\n",
        "! pip install python-box"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/6a/7bde208d21b4f7db1b666739dca8826d8fd7af34a14fddfda9b597ab8a45/contractions-0.0.23-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.0MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 41.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81709 sha256=423037c36cf004feb1bc13d02ddb95edc510153821eca2e3f321136801a47a60\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.23 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Collecting python-box\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/97/a8471260a7132345037804b0f2408b1dba9a7b8a58011b9f3ea3ab5e0f1d/python_box-3.4.6-py2.py3-none-any.whl\n",
            "Installing collected packages: python-box\n",
            "Successfully installed python-box-3.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hofDDHRX2zGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yx73Tlo20Hk",
        "colab_type": "code",
        "outputId": "b7ea4299-8402-4b41-b7ae-7700bd8bc115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwVEZOoptGCy",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Take a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggsYHVf221U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir = \"/content/drive/My Drive/ML-data/tweetsense/sentiment140/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO-1m1RW221N",
        "colab_type": "code",
        "outputId": "c9b11132-7b76-4464-99c4-4bd6a8c4ec5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "col_names = ['label', 'id', 'date', 'flag', 'user', 'text']\n",
        "df = pd.read_csv(dir + '/training.csv', encoding='latin-1', names=col_names)\n",
        "df = df[['text', 'label']]\n",
        "print(df.head(2))\n",
        "print(\"\\nThere are {} tweets in total.\\n\".format(len(df)))\n",
        "print(\"There are {} positve tweets\".format(np.sum(df[\"label\"] == 4)))\n",
        "print(\"There are {} neutral tweets\".format(np.sum(df[\"label\"] == 2)))\n",
        "print(\"There are {} negative tweets\".format(np.sum(df[\"label\"] == 0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  label\n",
            "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...      0\n",
            "1  is upset that he can't update his Facebook by ...      0\n",
            "\n",
            "There are 1600000 tweets in total.\n",
            "\n",
            "There are 800000 positve tweets\n",
            "There are 0 neutral tweets\n",
            "There are 800000 negative tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxYM1oG2tScF",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Define Preprocess steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5GiRIPbo7Sy",
        "colab_type": "code",
        "outputId": "45df24ce-4998-4be2-a169-f3356430a040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def remove_html(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    return souped\n",
        "\n",
        "text = \"&amp; I like it\"\n",
        "remove_html(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'& I like it'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUhEwbBGo_de",
        "colab_type": "code",
        "outputId": "149f07dd-00f6-4ded-fd6d-ea4d89bad840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def remove_url(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "text = \"Check this video http://bit.ly/IMXUM\"\n",
        "remove_url(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Check this video '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hXJ7aHDo3bq",
        "colab_type": "code",
        "outputId": "7b29b57a-ddaa-4be7-ff56-9a5a9efa83bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "def remove_mention(text):\n",
        "    return re.sub('@[^\\s]+','',text)\n",
        "\n",
        "text = \"@David hi\"\n",
        "remove_mention(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' hi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgAC-qD9pQX_",
        "colab_type": "code",
        "outputId": "bd18d629-c276-4c5b-e861-b8ae05237b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def remove_hashtag(text):\n",
        "    return re.sub(r'#\\w*', '', text)\n",
        "\n",
        "text = \"#trumpisidiot hahah\"\n",
        "remove_hashtag(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' hahah'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfrncEVkpU1B",
        "colab_type": "code",
        "outputId": "14dbac5f-0253-4788-9fef-be723d92c02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "text = 'string with \"punctuation\" inside of it! Does this work? I hope so.'\n",
        "remove_punctuation(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'string with punctuation inside of it Does this work I hope so'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-roO-CgpW6O",
        "colab_type": "code",
        "outputId": "f0363f88-a538-427b-b727-454e871c3d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def fix_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "text = \"I can't help you!\"\n",
        "fix_contractions(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I can not help you!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFQRe3lZpqxz",
        "colab_type": "code",
        "outputId": "78a267f7-cfba-4bd1-d8fc-ef576e2ec587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def preprocess(text):\n",
        "    text = remove_html(text)\n",
        "    text = remove_mention(text)\n",
        "    text = remove_url(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = fix_contractions(text)\n",
        "\n",
        "    words = TweetTokenizer().tokenize(text)\n",
        "    words = [word.lower() for word in words]\n",
        "    # Remove punctuation\n",
        "    words = [remove_punctuation(word) for word in words]\n",
        "    # Remove words that contain numeric values\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    \n",
        "    return \" \".join(words)\n",
        "\n",
        "text = \"\"\"\n",
        "boy's cars\n",
        "I can't help you. #Fucker. \n",
        "Check this out http://shit_url\n",
        "Lebron..sh*t was hilarious...LMAO!!!\n",
        "\"\"\"\n",
        "preprocess(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'boys cars i can not help you check this out lebron sh t was hilarious lmao'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFdwWEG-tcY2",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: Preprocess training, val and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBui0BlttrI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isfile(\"/content/drive/My Drive/ML-data/tweetsense/preprocessed_data/train.csv\"):\n",
        "    index = 0\n",
        "    for i, row in df.iterrows():\n",
        "        if index % 1000 == 0:\n",
        "            print(\"Finished processing {} training data\".format(index))\n",
        "        index += 1\n",
        "        text = df.at[i,'text']\n",
        "        text = preprocess(text)\n",
        "        df.at[i,'text'] = text\n",
        "else:\n",
        "    df = pd.read_csv(\n",
        "        \"/content/drive/My Drive/ML-data/tweetsense/preprocessed_data/train.csv\", \n",
        "        encoding='latin-1'\n",
        "    )\n",
        "    df = df.dropna(axis = 0, how ='any')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3iTNXoNwt1U",
        "colab_type": "code",
        "outputId": "d273f7ff-29de-49e8-d177-cfe840a9a8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>awww that is a bummer you shoulda got david ca...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is upset that he can not update his facebook b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i dived many times for the ball managed to sav...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no it is not behaving at all i am mad why am i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  awww that is a bummer you shoulda got david ca...      0\n",
              "1  is upset that he can not update his facebook b...      0\n",
              "2  i dived many times for the ball managed to sav...      0\n",
              "3     my whole body feels itchy and like its on fire      0\n",
              "4  no it is not behaving at all i am mad why am i...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpdWyn_tQfQ",
        "colab_type": "code",
        "outputId": "b6b70791-6528-4f13-f044-94dfbdd527b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train, val = train_test_split(df, test_size=0.3)\n",
        "print(\"Training set size: {}\".format(len(train)))\n",
        "print(\"Validation set size: {}\".format(len(val)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size: 1117502\n",
            "Validation set size: 478930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGc_12Qb1bl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('/content/drive/My Drive/ML-data/tweetsense/data/train.csv', index=False)\n",
        "val.to_csv('/content/drive/My Drive/ML-data/tweetsense/data/val.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF9caPo1v239",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Set up Bert Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2MusMjr3Hob",
        "colab_type": "code",
        "outputId": "307edfa4-035e-4fd1-92e5-be21649f9114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "! nvcc -V"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCqR-ixY3J_r",
        "colab_type": "code",
        "outputId": "12059cd6-174a-4ea3-c8fb-f603012e76b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vMKb3gu3LYo",
        "colab_type": "code",
        "outputId": "8d562374-ff5f-4547-ec9d-2a4f451b7f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHALX1OC3NIC",
        "colab_type": "code",
        "outputId": "d7f0a978-0349-463d-96e7-a69920d100da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install pytorch-pretrained-bert\n",
        "! pip install fast_bert\n",
        "! pip install pytorch-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.10.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.13.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-pretrained-bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.18->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n",
            "Collecting fast_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/60/69eabd53788e78dd07b6695ba9e02d96e9791dfcae3a5cc19365fe94254a/fast_bert-1.5.0-py3-none-any.whl (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n",
            "\u001b[?25hCollecting transformers>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/e7/0a1babead1b79afabb654fbec0a052e0d833ba4205a6dfd98b1aeda9c82e/transformers-2.2.0-py3-none-any.whl (360kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 7.9MB/s \n",
            "\u001b[?25hCollecting pytorch-lamb\n",
            "  Downloading https://files.pythonhosted.org/packages/43/98/3bce14a319317a2856db722f2542d329baf42845fa53563d0d749c5a2d40/pytorch_lamb-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from fast_bert) (0.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from fast_bert) (1.3.1)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->fast_bert) (1.17.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 46.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->fast_bert) (2019.11.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->fast_bert) (1.10.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->fast_bert) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->fast_bert) (2.21.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pytorch-lamb->fast_bert) (0.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->fast_bert) (0.21.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->fast_bert) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->fast_bert) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.2.0->fast_bert) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.2.0->fast_bert) (0.14.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->fast_bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->fast_bert) (1.13.18)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->fast_bert) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.2.0->fast_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.2.0->fast_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.2.0->fast_bert) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.2.0->fast_bert) (2.8)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pytorch-lamb->fast_bert) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->fast_bert) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX->fast_bert) (41.6.0)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers>=2.2.0->fast_bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers>=2.2.0->fast_bert) (0.15.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->pytorch-lamb->fast_bert) (0.46)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=e676118a575c5f234c5849b8dc7c2e043132b4356cc72bc1a0abf12c5710ef1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers, tensorboardX, pytorch-lamb, fast-bert\n",
            "Successfully installed fast-bert-1.5.0 pytorch-lamb-1.0.0 sacremoses-0.0.35 sentencepiece-0.1.83 tensorboardX-1.9 transformers-2.2.0\n",
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.11.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.83)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.35)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.10.18)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.13.18)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->pytorch-transformers) (0.15.2)\n",
            "Installing collected packages: pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mtyqV5zKwfU",
        "colab_type": "code",
        "outputId": "be31815e-a904-4fa0-82cf-bab261ada51f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/content/apex\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-gc3q2fwy\n",
            "Created temporary directory: /tmp/pip-req-tracker-2h3jutje\n",
            "Created requirements tracker '/tmp/pip-req-tracker-2h3jutje'\n",
            "Created temporary directory: /tmp/pip-install-0qykgzqx\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-4j4sbdd1\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-2h3jutje'\n",
            "    Running setup.py (path:/tmp/pip-req-build-4j4sbdd1/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "    torch.__version__  =  1.3.1\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-4j4sbdd1/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-4j4sbdd1/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-4j4sbdd1/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-4j4sbdd1/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-4j4sbdd1/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-4j4sbdd1/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-4j4sbdd1/setup.py:43: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-4j4sbdd1 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-2h3jutje'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /tmp/pip-uninstall-yb2dpefr\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex-0.1-py3.6.egg-info\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex/\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "      Successfully uninstalled apex-0.1\n",
            "  Created temporary directory: /tmp/pip-record-ua6x97r3\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-4j4sbdd1/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-4j4sbdd1/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-ua6x97r3/install-record.txt --single-version-externally-managed --compile\n",
            "    torch.__version__  =  1.3.1\n",
            "    /tmp/pip-req-build-4j4sbdd1/setup.py:43: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "    Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "    Cuda compilation tools, release 10.0, V10.0.130\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    running build_ext\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++11\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-ua6x97r3/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-4j4sbdd1\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-2h3jutje'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyfkz1uXwLg5",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Set up Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3dToMy73QLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import apex\n",
        "from fast_bert.data_cls import BertDataBunch\n",
        "from fast_bert.learner_cls import BertLearner\n",
        "from fast_bert.metrics import accuracy\n",
        "from box import Box"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G8D8ISB3WV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Box({\n",
        "    \"model_name\": 'bert-base-uncased',\n",
        "    \"model_type\": 'bert',\n",
        "    \"max_seq_length\": 128,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"learning_rate\": 6e-5,\n",
        "    \"num_train_epochs\": 6,\n",
        "    \"warmup_steps\": 500,\n",
        "    \"fp16\": True,\n",
        "    \"logging_steps\": 50,\n",
        "    \"multi_gpu\": True if torch.cuda.device_count() > 1 else False\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTW7b8YPILUg",
        "colab_type": "code",
        "outputId": "1b390012-d7c1-4881-9162-32bccd6bfb0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "    datefmt='%m/%d/%Y %H:%M:%S'\n",
        ")\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.info(args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 06:29:00 - INFO - root -   {'model_name': 'bert-base-uncased', 'model_type': 'bert', 'max_seq_length': 128, 'train_batch_size': 32, 'learning_rate': 6e-05, 'num_train_epochs': 6, 'warmup_steps': 500, 'fp16': True, 'logging_steps': 50, 'multi_gpu': False}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnV1IEyl5zYI",
        "colab_type": "code",
        "outputId": "4a4c688f-1472-479c-d5e3-eebb32c7cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pathlib import Path\n",
        "DATA_PATH = Path('/content/drive/My Drive/ML-data/tweetsense/data/')     # path for data files (train and val)\n",
        "LABEL_PATH = Path('/content/drive/My Drive/ML-data/tweetsense/labels/')  # path for labels file\n",
        "MODEL_PATH=Path('/content/drive/My Drive/ML-data/tweetsense/models/')    # path for model artifacts to be stored\n",
        "LOG_PATH=Path('/content/drive/My Drive/ML-data/tweetsense/logs/')       # path for log files to be stored\n",
        "print(DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ML-data/tweetsense/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tnwI8o55LEO",
        "colab_type": "code",
        "outputId": "10e7165a-1900-449b-b409-cc4ad6d80cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "databunch = BertDataBunch(\n",
        "    DATA_PATH, \n",
        "    LABEL_PATH, \n",
        "    args.model_name,\n",
        "    train_file='train.csv', \n",
        "    val_file='val.csv', \n",
        "    label_file='labels.csv',\n",
        "    text_col='text', \n",
        "    label_col='label',\n",
        "    batch_size_per_gpu=args.train_batch_size, \n",
        "    max_seq_length=args.max_seq_length, \n",
        "    multi_gpu=args.multi_gpu, \n",
        "    multi_label=False,\n",
        "    model_type=args.model_type\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 06:29:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/01/2019 06:29:02 - INFO - root -   Loading features from cached file /content/drive/My Drive/ML-data/tweetsense/data/cache/cached_bert_train_multi_class_128_train.csv\n",
            "12/01/2019 06:29:52 - INFO - root -   Loading features from cached file /content/drive/My Drive/ML-data/tweetsense/data/cache/cached_bert_dev_multi_class_128_val.csv\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7iwohvu5uBr",
        "colab_type": "code",
        "outputId": "201ceceb-4e1e-4469-f3fd-49a158c170e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "device_cuda = torch.device(\"cuda\")\n",
        "metrics = [{'name': 'accuracy', 'function': accuracy}]\n",
        "\n",
        "# The learner contains the logic for training loop, validation loop, \n",
        "# optimiser strategies and key metrics calculation\n",
        "learner = BertLearner.from_pretrained_model(\n",
        "    databunch, \n",
        "    pretrained_path=args.model_name, \n",
        "    metrics=metrics, \n",
        "    device=device_cuda, \n",
        "    logger=logger, \n",
        "    output_dir=\"/content/drive/My Drive/ML-data/tweetsense/output\",\n",
        "    finetuned_wgts_path=None, \n",
        "    warmup_steps=args.warmup_steps,\n",
        "    is_fp16=args.fp16,\n",
        "    multi_gpu=args.multi_gpu, \n",
        "    multi_label=False,\n",
        "    logging_steps=args.logging_steps\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 06:30:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "12/01/2019 06:30:11 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/01/2019 06:30:12 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/01/2019 06:30:15 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/01/2019 06:30:15 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACE3D6tyLmKB",
        "colab_type": "text"
      },
      "source": [
        "### Step 6: Train Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdQMk8V4Ip9-",
        "colab_type": "code",
        "outputId": "23b6a1dc-23be-4028-f9bc-fa3894f4780c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learner.fit(\n",
        "    epochs=1,\n",
        "\tlr=6e-5,\n",
        "\tvalidate=False, \t# Evaluate the model after each epoch\n",
        "\tschedule_type=\"warmup_cosine\",\n",
        "    optimizer_type=\"lamb\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 06:47:47 - INFO - root -   ***** Running training *****\n",
            "12/01/2019 06:47:47 - INFO - root -     Num examples = 1117502\n",
            "12/01/2019 06:47:47 - INFO - root -     Num Epochs = 1\n",
            "12/01/2019 06:47:47 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "12/01/2019 06:47:47 - INFO - root -     Gradient Accumulation steps = 1\n",
            "12/01/2019 06:47:47 - INFO - root -     Total optimization steps = 34922\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ML-data/tweetsense/output/tensorboard\n",
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 06:48:16 - INFO - root -   lr after step 50: 6e-06\n",
            "12/01/2019 06:48:16 - INFO - root -   train_loss after step 50: 0.7025802683830261\n",
            "12/01/2019 06:48:44 - INFO - root -   lr after step 100: 1.2e-05\n",
            "12/01/2019 06:48:44 - INFO - root -   train_loss after step 100: 0.6995965993404388\n",
            "12/01/2019 06:49:12 - INFO - root -   lr after step 150: 1.8e-05\n",
            "12/01/2019 06:49:12 - INFO - root -   train_loss after step 150: 0.6899114167690277\n",
            "12/01/2019 06:49:40 - INFO - root -   lr after step 200: 2.4e-05\n",
            "12/01/2019 06:49:40 - INFO - root -   train_loss after step 200: 0.6824549758434295\n",
            "12/01/2019 06:50:08 - INFO - root -   lr after step 250: 3e-05\n",
            "12/01/2019 06:50:08 - INFO - root -   train_loss after step 250: 0.671521338224411\n",
            "12/01/2019 06:50:36 - INFO - root -   lr after step 300: 3.6e-05\n",
            "12/01/2019 06:50:36 - INFO - root -   train_loss after step 300: 0.6529994702339172\n",
            "12/01/2019 06:51:05 - INFO - root -   lr after step 350: 4.2e-05\n",
            "12/01/2019 06:51:05 - INFO - root -   train_loss after step 350: 0.6441703903675079\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 06:51:33 - INFO - root -   lr after step 400: 4.8e-05\n",
            "12/01/2019 06:51:33 - INFO - root -   train_loss after step 400: 0.6191174590587616\n",
            "12/01/2019 06:52:01 - INFO - root -   lr after step 450: 5.4000000000000005e-05\n",
            "12/01/2019 06:52:01 - INFO - root -   train_loss after step 450: 0.5952912652492524\n",
            "12/01/2019 06:52:29 - INFO - root -   lr after step 500: 6e-05\n",
            "12/01/2019 06:52:29 - INFO - root -   train_loss after step 500: 0.5581774938106537\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 06:52:57 - INFO - root -   lr after step 550: 5.9999687638097775e-05\n",
            "12/01/2019 06:52:57 - INFO - root -   train_loss after step 550: 0.5143962901830673\n",
            "12/01/2019 06:53:26 - INFO - root -   lr after step 600: 5.999875055889577e-05\n",
            "12/01/2019 06:53:26 - INFO - root -   train_loss after step 600: 0.4865010124444962\n",
            "12/01/2019 06:53:54 - INFO - root -   lr after step 650: 5.9997188781907846e-05\n",
            "12/01/2019 06:53:54 - INFO - root -   train_loss after step 650: 0.49226940333843233\n",
            "12/01/2019 06:54:22 - INFO - root -   lr after step 700: 5.9995002339656644e-05\n",
            "12/01/2019 06:54:22 - INFO - root -   train_loss after step 700: 0.4814205551147461\n",
            "12/01/2019 06:54:51 - INFO - root -   lr after step 750: 5.9992191277672904e-05\n",
            "12/01/2019 06:54:51 - INFO - root -   train_loss after step 750: 0.482695991396904\n",
            "12/01/2019 06:55:19 - INFO - root -   lr after step 800: 5.998875565449454e-05\n",
            "12/01/2019 06:55:19 - INFO - root -   train_loss after step 800: 0.48330775916576385\n",
            "12/01/2019 06:55:47 - INFO - root -   lr after step 850: 5.998469554166542e-05\n",
            "12/01/2019 06:55:47 - INFO - root -   train_loss after step 850: 0.44570373237133026\n",
            "12/01/2019 06:56:16 - INFO - root -   lr after step 900: 5.998001102373383e-05\n",
            "12/01/2019 06:56:16 - INFO - root -   train_loss after step 900: 0.4551839083433151\n",
            "12/01/2019 06:56:44 - INFO - root -   lr after step 950: 5.997470219825078e-05\n",
            "12/01/2019 06:56:44 - INFO - root -   train_loss after step 950: 0.45033527493476866\n",
            "12/01/2019 06:57:12 - INFO - root -   lr after step 1000: 5.9968769175767905e-05\n",
            "12/01/2019 06:57:12 - INFO - root -   train_loss after step 1000: 0.46464277267456056\n",
            "12/01/2019 06:57:40 - INFO - root -   lr after step 1050: 5.996221207983523e-05\n",
            "12/01/2019 06:57:40 - INFO - root -   train_loss after step 1050: 0.4489255553483963\n",
            "12/01/2019 06:58:09 - INFO - root -   lr after step 1100: 5.995503104699857e-05\n",
            "12/01/2019 06:58:09 - INFO - root -   train_loss after step 1100: 0.45173012375831606\n",
            "12/01/2019 06:58:37 - INFO - root -   lr after step 1150: 5.994722622679663e-05\n",
            "12/01/2019 06:58:37 - INFO - root -   train_loss after step 1150: 0.42010215401649476\n",
            "12/01/2019 06:59:05 - INFO - root -   lr after step 1200: 5.9938797781757994e-05\n",
            "12/01/2019 06:59:05 - INFO - root -   train_loss after step 1200: 0.4571256083250046\n",
            "12/01/2019 06:59:34 - INFO - root -   lr after step 1250: 5.992974588739768e-05\n",
            "12/01/2019 06:59:34 - INFO - root -   train_loss after step 1250: 0.4590353435277939\n",
            "12/01/2019 07:00:02 - INFO - root -   lr after step 1300: 5.992007073221346e-05\n",
            "12/01/2019 07:00:02 - INFO - root -   train_loss after step 1300: 0.44421485662460325\n",
            "12/01/2019 07:00:30 - INFO - root -   lr after step 1350: 5.990977251768201e-05\n",
            "12/01/2019 07:00:30 - INFO - root -   train_loss after step 1350: 0.42661827206611636\n",
            "12/01/2019 07:00:59 - INFO - root -   lr after step 1400: 5.9898851458254646e-05\n",
            "12/01/2019 07:00:59 - INFO - root -   train_loss after step 1400: 0.44005006790161133\n",
            "12/01/2019 07:01:27 - INFO - root -   lr after step 1450: 5.9887307781352905e-05\n",
            "12/01/2019 07:01:27 - INFO - root -   train_loss after step 1450: 0.43293018847703935\n",
            "12/01/2019 07:01:55 - INFO - root -   lr after step 1500: 5.987514172736376e-05\n",
            "12/01/2019 07:01:55 - INFO - root -   train_loss after step 1500: 0.4098013183474541\n",
            "12/01/2019 07:02:23 - INFO - root -   lr after step 1550: 5.986235354963469e-05\n",
            "12/01/2019 07:02:23 - INFO - root -   train_loss after step 1550: 0.4449631780385971\n",
            "12/01/2019 07:02:52 - INFO - root -   lr after step 1600: 5.98489435144683e-05\n",
            "12/01/2019 07:02:52 - INFO - root -   train_loss after step 1600: 0.42892065048217776\n",
            "12/01/2019 07:03:20 - INFO - root -   lr after step 1650: 5.9834911901116884e-05\n",
            "12/01/2019 07:03:20 - INFO - root -   train_loss after step 1650: 0.4114029490947723\n",
            "12/01/2019 07:03:48 - INFO - root -   lr after step 1700: 5.982025900177653e-05\n",
            "12/01/2019 07:03:48 - INFO - root -   train_loss after step 1700: 0.42324944972991946\n",
            "12/01/2019 07:04:16 - INFO - root -   lr after step 1750: 5.9804985121581076e-05\n",
            "12/01/2019 07:04:16 - INFO - root -   train_loss after step 1750: 0.43501880168914797\n",
            "12/01/2019 07:04:44 - INFO - root -   lr after step 1800: 5.978909057859574e-05\n",
            "12/01/2019 07:04:44 - INFO - root -   train_loss after step 1800: 0.397387789785862\n",
            "12/01/2019 07:05:13 - INFO - root -   lr after step 1850: 5.9772575703810487e-05\n",
            "12/01/2019 07:05:13 - INFO - root -   train_loss after step 1850: 0.41002316802740096\n",
            "12/01/2019 07:05:41 - INFO - root -   lr after step 1900: 5.975544084113318e-05\n",
            "12/01/2019 07:05:41 - INFO - root -   train_loss after step 1900: 0.40436884164810183\n",
            "12/01/2019 07:06:09 - INFO - root -   lr after step 1950: 5.9737686347382365e-05\n",
            "12/01/2019 07:06:09 - INFO - root -   train_loss after step 1950: 0.4129583865404129\n",
            "12/01/2019 07:06:37 - INFO - root -   lr after step 2000: 5.9719312592279876e-05\n",
            "12/01/2019 07:06:37 - INFO - root -   train_loss after step 2000: 0.4293571665883064\n",
            "12/01/2019 07:07:05 - INFO - root -   lr after step 2050: 5.970031995844311e-05\n",
            "12/01/2019 07:07:05 - INFO - root -   train_loss after step 2050: 0.40576727509498595\n",
            "12/01/2019 07:07:34 - INFO - root -   lr after step 2100: 5.96807088413771e-05\n",
            "12/01/2019 07:07:34 - INFO - root -   train_loss after step 2100: 0.4127191171050072\n",
            "12/01/2019 07:08:02 - INFO - root -   lr after step 2150: 5.966047964946621e-05\n",
            "12/01/2019 07:08:02 - INFO - root -   train_loss after step 2150: 0.43181368231773376\n",
            "12/01/2019 07:08:30 - INFO - root -   lr after step 2200: 5.963963280396573e-05\n",
            "12/01/2019 07:08:30 - INFO - root -   train_loss after step 2200: 0.41432862371206286\n",
            "12/01/2019 07:08:58 - INFO - root -   lr after step 2250: 5.9618168738992984e-05\n",
            "12/01/2019 07:08:58 - INFO - root -   train_loss after step 2250: 0.413280985057354\n",
            "12/01/2019 07:09:26 - INFO - root -   lr after step 2300: 5.959608790151839e-05\n",
            "12/01/2019 07:09:26 - INFO - root -   train_loss after step 2300: 0.40202122926712036\n",
            "12/01/2019 07:09:55 - INFO - root -   lr after step 2350: 5.9573390751356124e-05\n",
            "12/01/2019 07:09:55 - INFO - root -   train_loss after step 2350: 0.41816133707761766\n",
            "12/01/2019 07:10:23 - INFO - root -   lr after step 2400: 5.955007776115451e-05\n",
            "12/01/2019 07:10:23 - INFO - root -   train_loss after step 2400: 0.42653142154216767\n",
            "12/01/2019 07:10:51 - INFO - root -   lr after step 2450: 5.95261494163862e-05\n",
            "12/01/2019 07:10:51 - INFO - root -   train_loss after step 2450: 0.43738063335418703\n",
            "12/01/2019 07:11:19 - INFO - root -   lr after step 2500: 5.95016062153381e-05\n",
            "12/01/2019 07:11:19 - INFO - root -   train_loss after step 2500: 0.426930747628212\n",
            "12/01/2019 07:11:48 - INFO - root -   lr after step 2550: 5.947644866910093e-05\n",
            "12/01/2019 07:11:48 - INFO - root -   train_loss after step 2550: 0.40711925536394117\n",
            "12/01/2019 07:12:16 - INFO - root -   lr after step 2600: 5.945067730155862e-05\n",
            "12/01/2019 07:12:16 - INFO - root -   train_loss after step 2600: 0.40806208342313766\n",
            "12/01/2019 07:12:44 - INFO - root -   lr after step 2650: 5.942429264937739e-05\n",
            "12/01/2019 07:12:44 - INFO - root -   train_loss after step 2650: 0.39677264660596845\n",
            "12/01/2019 07:13:12 - INFO - root -   lr after step 2700: 5.939729526199462e-05\n",
            "12/01/2019 07:13:12 - INFO - root -   train_loss after step 2700: 0.41074174344539643\n",
            "12/01/2019 07:13:41 - INFO - root -   lr after step 2750: 5.9369685701607284e-05\n",
            "12/01/2019 07:13:41 - INFO - root -   train_loss after step 2750: 0.39567023813724517\n",
            "12/01/2019 07:14:09 - INFO - root -   lr after step 2800: 5.9341464543160395e-05\n",
            "12/01/2019 07:14:09 - INFO - root -   train_loss after step 2800: 0.38308647245168687\n",
            "12/01/2019 07:14:37 - INFO - root -   lr after step 2850: 5.931263237433492e-05\n",
            "12/01/2019 07:14:37 - INFO - root -   train_loss after step 2850: 0.42680847108364106\n",
            "12/01/2019 07:15:05 - INFO - root -   lr after step 2900: 5.928318979553561e-05\n",
            "12/01/2019 07:15:05 - INFO - root -   train_loss after step 2900: 0.39557625621557235\n",
            "12/01/2019 07:15:34 - INFO - root -   lr after step 2950: 5.925313741987846e-05\n",
            "12/01/2019 07:15:34 - INFO - root -   train_loss after step 2950: 0.4172488358616829\n",
            "12/01/2019 07:16:02 - INFO - root -   lr after step 3000: 5.9222475873177936e-05\n",
            "12/01/2019 07:16:02 - INFO - root -   train_loss after step 3000: 0.4059688073396683\n",
            "12/01/2019 07:16:30 - INFO - root -   lr after step 3050: 5.9191205793934e-05\n",
            "12/01/2019 07:16:30 - INFO - root -   train_loss after step 3050: 0.4044210723042488\n",
            "12/01/2019 07:16:58 - INFO - root -   lr after step 3100: 5.915932783331873e-05\n",
            "12/01/2019 07:16:58 - INFO - root -   train_loss after step 3100: 0.3953260827064514\n",
            "12/01/2019 07:17:27 - INFO - root -   lr after step 3150: 5.9126842655162827e-05\n",
            "12/01/2019 07:17:27 - INFO - root -   train_loss after step 3150: 0.41948847740888595\n",
            "12/01/2019 07:17:55 - INFO - root -   lr after step 3200: 5.909375093594175e-05\n",
            "12/01/2019 07:17:55 - INFO - root -   train_loss after step 3200: 0.3938970497250557\n",
            "12/01/2019 07:18:24 - INFO - root -   lr after step 3250: 5.9060053364761666e-05\n",
            "12/01/2019 07:18:24 - INFO - root -   train_loss after step 3250: 0.4161321720480919\n",
            "12/01/2019 07:18:52 - INFO - root -   lr after step 3300: 5.902575064334508e-05\n",
            "12/01/2019 07:18:52 - INFO - root -   train_loss after step 3300: 0.40299391210079194\n",
            "12/01/2019 07:19:20 - INFO - root -   lr after step 3350: 5.8990843486016184e-05\n",
            "12/01/2019 07:19:20 - INFO - root -   train_loss after step 3350: 0.3930888769030571\n",
            "12/01/2019 07:19:48 - INFO - root -   lr after step 3400: 5.895533261968608e-05\n",
            "12/01/2019 07:19:48 - INFO - root -   train_loss after step 3400: 0.3838687962293625\n",
            "12/01/2019 07:20:17 - INFO - root -   lr after step 3450: 5.891921878383754e-05\n",
            "12/01/2019 07:20:17 - INFO - root -   train_loss after step 3450: 0.40626816093921664\n",
            "12/01/2019 07:20:45 - INFO - root -   lr after step 3500: 5.888250273050965e-05\n",
            "12/01/2019 07:20:45 - INFO - root -   train_loss after step 3500: 0.400652736723423\n",
            "12/01/2019 07:21:13 - INFO - root -   lr after step 3550: 5.8845185224282184e-05\n",
            "12/01/2019 07:21:13 - INFO - root -   train_loss after step 3550: 0.3891615059971809\n",
            "12/01/2019 07:21:42 - INFO - root -   lr after step 3600: 5.8807267042259604e-05\n",
            "12/01/2019 07:21:42 - INFO - root -   train_loss after step 3600: 0.4060422337055206\n",
            "12/01/2019 07:22:10 - INFO - root -   lr after step 3650: 5.876874897405495e-05\n",
            "12/01/2019 07:22:10 - INFO - root -   train_loss after step 3650: 0.39405794829130175\n",
            "12/01/2019 07:22:38 - INFO - root -   lr after step 3700: 5.872963182177335e-05\n",
            "12/01/2019 07:22:38 - INFO - root -   train_loss after step 3700: 0.37339523166418076\n",
            "12/01/2019 07:23:06 - INFO - root -   lr after step 3750: 5.8689916399995364e-05\n",
            "12/01/2019 07:23:06 - INFO - root -   train_loss after step 3750: 0.40238225132226946\n",
            "12/01/2019 07:23:35 - INFO - root -   lr after step 3800: 5.864960353575995e-05\n",
            "12/01/2019 07:23:35 - INFO - root -   train_loss after step 3800: 0.40694767236709595\n",
            "12/01/2019 07:24:03 - INFO - root -   lr after step 3850: 5.8608694068547324e-05\n",
            "12/01/2019 07:24:03 - INFO - root -   train_loss after step 3850: 0.4083766797184944\n",
            "12/01/2019 07:24:31 - INFO - root -   lr after step 3900: 5.8567188850261394e-05\n",
            "12/01/2019 07:24:31 - INFO - root -   train_loss after step 3900: 0.38134018033742906\n",
            "12/01/2019 07:25:00 - INFO - root -   lr after step 3950: 5.852508874521211e-05\n",
            "12/01/2019 07:25:00 - INFO - root -   train_loss after step 3950: 0.4160634922981262\n",
            "12/01/2019 07:25:28 - INFO - root -   lr after step 4000: 5.848239463009739e-05\n",
            "12/01/2019 07:25:28 - INFO - root -   train_loss after step 4000: 0.3773183389008045\n",
            "12/01/2019 07:25:56 - INFO - root -   lr after step 4050: 5.843910739398492e-05\n",
            "12/01/2019 07:25:56 - INFO - root -   train_loss after step 4050: 0.37956483364105226\n",
            "12/01/2019 07:26:24 - INFO - root -   lr after step 4100: 5.839522793829356e-05\n",
            "12/01/2019 07:26:24 - INFO - root -   train_loss after step 4100: 0.3980813294649124\n",
            "12/01/2019 07:26:52 - INFO - root -   lr after step 4150: 5.835075717677469e-05\n",
            "12/01/2019 07:26:52 - INFO - root -   train_loss after step 4150: 0.42205611258745196\n",
            "12/01/2019 07:27:21 - INFO - root -   lr after step 4200: 5.830569603549306e-05\n",
            "12/01/2019 07:27:21 - INFO - root -   train_loss after step 4200: 0.39447938501834867\n",
            "12/01/2019 07:27:49 - INFO - root -   lr after step 4250: 5.8260045452807616e-05\n",
            "12/01/2019 07:27:49 - INFO - root -   train_loss after step 4250: 0.37896980464458463\n",
            "12/01/2019 07:28:17 - INFO - root -   lr after step 4300: 5.821380637935187e-05\n",
            "12/01/2019 07:28:17 - INFO - root -   train_loss after step 4300: 0.39248750150203704\n",
            "12/01/2019 07:28:45 - INFO - root -   lr after step 4350: 5.816697977801415e-05\n",
            "12/01/2019 07:28:45 - INFO - root -   train_loss after step 4350: 0.3951259660720825\n",
            "12/01/2019 07:29:13 - INFO - root -   lr after step 4400: 5.811956662391755e-05\n",
            "12/01/2019 07:29:13 - INFO - root -   train_loss after step 4400: 0.3834646254777908\n",
            "12/01/2019 07:29:42 - INFO - root -   lr after step 4450: 5.807156790439958e-05\n",
            "12/01/2019 07:29:42 - INFO - root -   train_loss after step 4450: 0.3905778607726097\n",
            "12/01/2019 07:30:10 - INFO - root -   lr after step 4500: 5.8022984618991694e-05\n",
            "12/01/2019 07:30:10 - INFO - root -   train_loss after step 4500: 0.3972955709695816\n",
            "12/01/2019 07:30:38 - INFO - root -   lr after step 4550: 5.7973817779398366e-05\n",
            "12/01/2019 07:30:38 - INFO - root -   train_loss after step 4550: 0.3734095552563667\n",
            "12/01/2019 07:31:06 - INFO - root -   lr after step 4600: 5.7924068409476105e-05\n",
            "12/01/2019 07:31:06 - INFO - root -   train_loss after step 4600: 0.4201881697773933\n",
            "12/01/2019 07:31:34 - INFO - root -   lr after step 4650: 5.7873737545212105e-05\n",
            "12/01/2019 07:31:34 - INFO - root -   train_loss after step 4650: 0.4021553874015808\n",
            "12/01/2019 07:32:03 - INFO - root -   lr after step 4700: 5.7822826234702654e-05\n",
            "12/01/2019 07:32:03 - INFO - root -   train_loss after step 4700: 0.3956382468342781\n",
            "12/01/2019 07:32:31 - INFO - root -   lr after step 4750: 5.777133553813135e-05\n",
            "12/01/2019 07:32:31 - INFO - root -   train_loss after step 4750: 0.4199379312992096\n",
            "12/01/2019 07:32:59 - INFO - root -   lr after step 4800: 5.771926652774698e-05\n",
            "12/01/2019 07:32:59 - INFO - root -   train_loss after step 4800: 0.3907957294583321\n",
            "12/01/2019 07:33:28 - INFO - root -   lr after step 4850: 5.766662028784123e-05\n",
            "12/01/2019 07:33:28 - INFO - root -   train_loss after step 4850: 0.39004536360502245\n",
            "12/01/2019 07:33:56 - INFO - root -   lr after step 4900: 5.761339791472607e-05\n",
            "12/01/2019 07:33:56 - INFO - root -   train_loss after step 4900: 0.38509247213602066\n",
            "12/01/2019 07:34:24 - INFO - root -   lr after step 4950: 5.755960051671094e-05\n",
            "12/01/2019 07:34:24 - INFO - root -   train_loss after step 4950: 0.37497752577066423\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 07:34:52 - INFO - root -   lr after step 5000: 5.750522921407969e-05\n",
            "12/01/2019 07:34:52 - INFO - root -   train_loss after step 5000: 0.3653702408075333\n",
            "12/01/2019 07:35:20 - INFO - root -   lr after step 5050: 5.745028513906721e-05\n",
            "12/01/2019 07:35:20 - INFO - root -   train_loss after step 5050: 0.36917432487010954\n",
            "12/01/2019 07:35:49 - INFO - root -   lr after step 5100: 5.7394769435835894e-05\n",
            "12/01/2019 07:35:49 - INFO - root -   train_loss after step 5100: 0.38490713000297544\n",
            "12/01/2019 07:36:17 - INFO - root -   lr after step 5150: 5.7338683260451776e-05\n",
            "12/01/2019 07:36:17 - INFO - root -   train_loss after step 5150: 0.37509211331605913\n",
            "12/01/2019 07:36:46 - INFO - root -   lr after step 5200: 5.728202778086051e-05\n",
            "12/01/2019 07:36:46 - INFO - root -   train_loss after step 5200: 0.359817915558815\n",
            "12/01/2019 07:37:14 - INFO - root -   lr after step 5250: 5.722480417686295e-05\n",
            "12/01/2019 07:37:14 - INFO - root -   train_loss after step 5250: 0.3762639024853706\n",
            "12/01/2019 07:37:42 - INFO - root -   lr after step 5300: 5.716701364009072e-05\n",
            "12/01/2019 07:37:42 - INFO - root -   train_loss after step 5300: 0.3917141020298004\n",
            "12/01/2019 07:38:10 - INFO - root -   lr after step 5350: 5.710865737398126e-05\n",
            "12/01/2019 07:38:10 - INFO - root -   train_loss after step 5350: 0.4118698814511299\n",
            "12/01/2019 07:38:39 - INFO - root -   lr after step 5400: 5.704973659375287e-05\n",
            "12/01/2019 07:38:39 - INFO - root -   train_loss after step 5400: 0.38658671736717226\n",
            "12/01/2019 07:39:07 - INFO - root -   lr after step 5450: 5.6990252526379346e-05\n",
            "12/01/2019 07:39:07 - INFO - root -   train_loss after step 5450: 0.4026411572098732\n",
            "12/01/2019 07:39:35 - INFO - root -   lr after step 5500: 5.693020641056445e-05\n",
            "12/01/2019 07:39:35 - INFO - root -   train_loss after step 5500: 0.37211779177188875\n",
            "12/01/2019 07:40:03 - INFO - root -   lr after step 5550: 5.686959949671611e-05\n",
            "12/01/2019 07:40:03 - INFO - root -   train_loss after step 5550: 0.41100876331329345\n",
            "12/01/2019 07:40:31 - INFO - root -   lr after step 5600: 5.680843304692038e-05\n",
            "12/01/2019 07:40:31 - INFO - root -   train_loss after step 5600: 0.3687173464894295\n",
            "12/01/2019 07:41:00 - INFO - root -   lr after step 5650: 5.6746708334915186e-05\n",
            "12/01/2019 07:41:00 - INFO - root -   train_loss after step 5650: 0.3818229818344116\n",
            "12/01/2019 07:41:28 - INFO - root -   lr after step 5700: 5.6684426646063735e-05\n",
            "12/01/2019 07:41:28 - INFO - root -   train_loss after step 5700: 0.38480963587760925\n",
            "12/01/2019 07:41:56 - INFO - root -   lr after step 5750: 5.662158927732784e-05\n",
            "12/01/2019 07:41:56 - INFO - root -   train_loss after step 5750: 0.37333155661821366\n",
            "12/01/2019 07:42:24 - INFO - root -   lr after step 5800: 5.655819753724082e-05\n",
            "12/01/2019 07:42:24 - INFO - root -   train_loss after step 5800: 0.3912949021160603\n",
            "12/01/2019 07:42:52 - INFO - root -   lr after step 5850: 5.649425274588031e-05\n",
            "12/01/2019 07:42:52 - INFO - root -   train_loss after step 5850: 0.3810656052827835\n",
            "12/01/2019 07:43:21 - INFO - root -   lr after step 5900: 5.642975623484075e-05\n",
            "12/01/2019 07:43:21 - INFO - root -   train_loss after step 5900: 0.37078978776931765\n",
            "12/01/2019 07:43:49 - INFO - root -   lr after step 5950: 5.636470934720567e-05\n",
            "12/01/2019 07:43:49 - INFO - root -   train_loss after step 5950: 0.3819247248768807\n",
            "12/01/2019 07:44:17 - INFO - root -   lr after step 6000: 5.629911343751972e-05\n",
            "12/01/2019 07:44:17 - INFO - root -   train_loss after step 6000: 0.3678262695670128\n",
            "12/01/2019 07:44:45 - INFO - root -   lr after step 6050: 5.6232969871760424e-05\n",
            "12/01/2019 07:44:45 - INFO - root -   train_loss after step 6050: 0.3951081410050392\n",
            "12/01/2019 07:45:14 - INFO - root -   lr after step 6100: 5.616628002730979e-05\n",
            "12/01/2019 07:45:14 - INFO - root -   train_loss after step 6100: 0.36628905177116394\n",
            "12/01/2019 07:45:42 - INFO - root -   lr after step 6150: 5.609904529292559e-05\n",
            "12/01/2019 07:45:42 - INFO - root -   train_loss after step 6150: 0.38844152599573134\n",
            "12/01/2019 07:46:10 - INFO - root -   lr after step 6200: 5.6031267068712475e-05\n",
            "12/01/2019 07:46:10 - INFO - root -   train_loss after step 6200: 0.39008799970149993\n",
            "12/01/2019 07:46:38 - INFO - root -   lr after step 6250: 5.596294676609277e-05\n",
            "12/01/2019 07:46:38 - INFO - root -   train_loss after step 6250: 0.39240866363048554\n",
            "12/01/2019 07:47:07 - INFO - root -   lr after step 6300: 5.589408580777713e-05\n",
            "12/01/2019 07:47:07 - INFO - root -   train_loss after step 6300: 0.3941660353541374\n",
            "12/01/2019 07:47:35 - INFO - root -   lr after step 6350: 5.582468562773486e-05\n",
            "12/01/2019 07:47:35 - INFO - root -   train_loss after step 6350: 0.3880791288614273\n",
            "12/01/2019 07:48:03 - INFO - root -   lr after step 6400: 5.575474767116413e-05\n",
            "12/01/2019 07:48:03 - INFO - root -   train_loss after step 6400: 0.39503567934036254\n",
            "12/01/2019 07:48:31 - INFO - root -   lr after step 6450: 5.568427339446181e-05\n",
            "12/01/2019 07:48:31 - INFO - root -   train_loss after step 6450: 0.3657560920715332\n",
            "12/01/2019 07:48:59 - INFO - root -   lr after step 6500: 5.561326426519318e-05\n",
            "12/01/2019 07:48:59 - INFO - root -   train_loss after step 6500: 0.4034618818759918\n",
            "12/01/2019 07:49:28 - INFO - root -   lr after step 6550: 5.554172176206135e-05\n",
            "12/01/2019 07:49:28 - INFO - root -   train_loss after step 6550: 0.3727321636676788\n",
            "12/01/2019 07:49:56 - INFO - root -   lr after step 6600: 5.546964737487648e-05\n",
            "12/01/2019 07:49:56 - INFO - root -   train_loss after step 6600: 0.35977739214897153\n",
            "12/01/2019 07:50:24 - INFO - root -   lr after step 6650: 5.539704260452475e-05\n",
            "12/01/2019 07:50:24 - INFO - root -   train_loss after step 6650: 0.39683116286993025\n",
            "12/01/2019 07:50:52 - INFO - root -   lr after step 6700: 5.5323908962937096e-05\n",
            "12/01/2019 07:50:52 - INFO - root -   train_loss after step 6700: 0.3924119618535042\n",
            "12/01/2019 07:51:21 - INFO - root -   lr after step 6750: 5.525024797305776e-05\n",
            "12/01/2019 07:51:21 - INFO - root -   train_loss after step 6750: 0.3901082906126976\n",
            "12/01/2019 07:51:49 - INFO - root -   lr after step 6800: 5.517606116881252e-05\n",
            "12/01/2019 07:51:49 - INFO - root -   train_loss after step 6800: 0.37158272802829745\n",
            "12/01/2019 07:52:17 - INFO - root -   lr after step 6850: 5.510135009507681e-05\n",
            "12/01/2019 07:52:17 - INFO - root -   train_loss after step 6850: 0.3846310529112816\n",
            "12/01/2019 07:52:45 - INFO - root -   lr after step 6900: 5.502611630764349e-05\n",
            "12/01/2019 07:52:45 - INFO - root -   train_loss after step 6900: 0.4047902935743332\n",
            "12/01/2019 07:53:14 - INFO - root -   lr after step 6950: 5.49503613731905e-05\n",
            "12/01/2019 07:53:14 - INFO - root -   train_loss after step 6950: 0.3987841981649399\n",
            "12/01/2019 07:53:42 - INFO - root -   lr after step 7000: 5.487408686924821e-05\n",
            "12/01/2019 07:53:42 - INFO - root -   train_loss after step 7000: 0.40395128339529035\n",
            "12/01/2019 07:54:11 - INFO - root -   lr after step 7050: 5.479729438416654e-05\n",
            "12/01/2019 07:54:11 - INFO - root -   train_loss after step 7050: 0.377304697483778\n",
            "12/01/2019 07:54:39 - INFO - root -   lr after step 7100: 5.471998551708196e-05\n",
            "12/01/2019 07:54:39 - INFO - root -   train_loss after step 7100: 0.3998971170186996\n",
            "12/01/2019 07:55:07 - INFO - root -   lr after step 7150: 5.46421618778841e-05\n",
            "12/01/2019 07:55:07 - INFO - root -   train_loss after step 7150: 0.382976176738739\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 07:55:35 - INFO - root -   lr after step 7200: 5.456382508718232e-05\n",
            "12/01/2019 07:55:35 - INFO - root -   train_loss after step 7200: 0.39777462124824525\n",
            "12/01/2019 07:56:04 - INFO - root -   lr after step 7250: 5.448497677627185e-05\n",
            "12/01/2019 07:56:04 - INFO - root -   train_loss after step 7250: 0.3725033676624298\n",
            "12/01/2019 07:56:32 - INFO - root -   lr after step 7300: 5.4405618587099944e-05\n",
            "12/01/2019 07:56:32 - INFO - root -   train_loss after step 7300: 0.4135435149073601\n",
            "12/01/2019 07:57:00 - INFO - root -   lr after step 7350: 5.432575217223159e-05\n",
            "12/01/2019 07:57:00 - INFO - root -   train_loss after step 7350: 0.37115988284349444\n",
            "12/01/2019 07:57:29 - INFO - root -   lr after step 7400: 5.424537919481512e-05\n",
            "12/01/2019 07:57:29 - INFO - root -   train_loss after step 7400: 0.3712014871835709\n",
            "12/01/2019 07:57:57 - INFO - root -   lr after step 7450: 5.4164501328547637e-05\n",
            "12/01/2019 07:57:57 - INFO - root -   train_loss after step 7450: 0.37656371742486955\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 07:58:25 - INFO - root -   lr after step 7500: 5.408312025764008e-05\n",
            "12/01/2019 07:58:25 - INFO - root -   train_loss after step 7500: 0.38841953247785566\n",
            "12/01/2019 07:58:54 - INFO - root -   lr after step 7550: 5.400123767678217e-05\n",
            "12/01/2019 07:58:54 - INFO - root -   train_loss after step 7550: 0.3785243484377861\n",
            "12/01/2019 07:59:22 - INFO - root -   lr after step 7600: 5.391885529110717e-05\n",
            "12/01/2019 07:59:22 - INFO - root -   train_loss after step 7600: 0.3681736424565315\n",
            "12/01/2019 07:59:50 - INFO - root -   lr after step 7650: 5.3835974816156326e-05\n",
            "12/01/2019 07:59:50 - INFO - root -   train_loss after step 7650: 0.3897144567966461\n",
            "12/01/2019 08:00:18 - INFO - root -   lr after step 7700: 5.375259797784315e-05\n",
            "12/01/2019 08:00:18 - INFO - root -   train_loss after step 7700: 0.4014959791302681\n",
            "12/01/2019 08:00:47 - INFO - root -   lr after step 7750: 5.3668726512417495e-05\n",
            "12/01/2019 08:00:47 - INFO - root -   train_loss after step 7750: 0.37495912730693814\n",
            "12/01/2019 08:01:15 - INFO - root -   lr after step 7800: 5.3584362166429415e-05\n",
            "12/01/2019 08:01:15 - INFO - root -   train_loss after step 7800: 0.3686438670754433\n",
            "12/01/2019 08:01:43 - INFO - root -   lr after step 7850: 5.3499506696692725e-05\n",
            "12/01/2019 08:01:43 - INFO - root -   train_loss after step 7850: 0.3793312081694603\n",
            "12/01/2019 08:02:12 - INFO - root -   lr after step 7900: 5.341416187024851e-05\n",
            "12/01/2019 08:02:12 - INFO - root -   train_loss after step 7900: 0.3801951426267624\n",
            "12/01/2019 08:02:40 - INFO - root -   lr after step 7950: 5.332832946432824e-05\n",
            "12/01/2019 08:02:40 - INFO - root -   train_loss after step 7950: 0.3764063882827759\n",
            "12/01/2019 08:03:08 - INFO - root -   lr after step 8000: 5.324201126631683e-05\n",
            "12/01/2019 08:03:08 - INFO - root -   train_loss after step 8000: 0.369513298869133\n",
            "12/01/2019 08:03:37 - INFO - root -   lr after step 8050: 5.315520907371537e-05\n",
            "12/01/2019 08:03:37 - INFO - root -   train_loss after step 8050: 0.3983121308684349\n",
            "12/01/2019 08:04:05 - INFO - root -   lr after step 8100: 5.3067924694103743e-05\n",
            "12/01/2019 08:04:05 - INFO - root -   train_loss after step 8100: 0.3786225774884224\n",
            "12/01/2019 08:04:33 - INFO - root -   lr after step 8150: 5.298015994510293e-05\n",
            "12/01/2019 08:04:33 - INFO - root -   train_loss after step 8150: 0.36767257511615753\n",
            "12/01/2019 08:05:01 - INFO - root -   lr after step 8200: 5.28919166543372e-05\n",
            "12/01/2019 08:05:01 - INFO - root -   train_loss after step 8200: 0.38905601561069486\n",
            "12/01/2019 08:05:29 - INFO - root -   lr after step 8250: 5.2803196659396025e-05\n",
            "12/01/2019 08:05:29 - INFO - root -   train_loss after step 8250: 0.38263689070940016\n",
            "12/01/2019 08:05:58 - INFO - root -   lr after step 8300: 5.2714001807795826e-05\n",
            "12/01/2019 08:05:58 - INFO - root -   train_loss after step 8300: 0.3845977386832237\n",
            "12/01/2019 08:06:26 - INFO - root -   lr after step 8350: 5.262433395694152e-05\n",
            "12/01/2019 08:06:26 - INFO - root -   train_loss after step 8350: 0.3810198502242565\n",
            "12/01/2019 08:06:54 - INFO - root -   lr after step 8400: 5.253419497408779e-05\n",
            "12/01/2019 08:06:54 - INFO - root -   train_loss after step 8400: 0.3808525687456131\n",
            "12/01/2019 08:07:22 - INFO - root -   lr after step 8450: 5.244358673630024e-05\n",
            "12/01/2019 08:07:22 - INFO - root -   train_loss after step 8450: 0.4013143143057823\n",
            "12/01/2019 08:07:50 - INFO - root -   lr after step 8500: 5.235251113041632e-05\n",
            "12/01/2019 08:07:50 - INFO - root -   train_loss after step 8500: 0.37878244698047636\n",
            "12/01/2019 08:08:18 - INFO - root -   lr after step 8550: 5.2260970053006005e-05\n",
            "12/01/2019 08:08:18 - INFO - root -   train_loss after step 8550: 0.3889413118362427\n",
            "12/01/2019 08:08:47 - INFO - root -   lr after step 8600: 5.2168965410332276e-05\n",
            "12/01/2019 08:08:47 - INFO - root -   train_loss after step 8600: 0.40049366623163224\n",
            "12/01/2019 08:09:15 - INFO - root -   lr after step 8650: 5.2076499118311474e-05\n",
            "12/01/2019 08:09:15 - INFO - root -   train_loss after step 8650: 0.38990521520376203\n",
            "12/01/2019 08:09:43 - INFO - root -   lr after step 8700: 5.1983573102473424e-05\n",
            "12/01/2019 08:09:43 - INFO - root -   train_loss after step 8700: 0.37749682754278185\n",
            "12/01/2019 08:10:11 - INFO - root -   lr after step 8750: 5.1890189297921244e-05\n",
            "12/01/2019 08:10:11 - INFO - root -   train_loss after step 8750: 0.36166304409503935\n",
            "12/01/2019 08:10:39 - INFO - root -   lr after step 8800: 5.179634964929112e-05\n",
            "12/01/2019 08:10:39 - INFO - root -   train_loss after step 8800: 0.38009062021970746\n",
            "12/01/2019 08:11:08 - INFO - root -   lr after step 8850: 5.170205611071181e-05\n",
            "12/01/2019 08:11:08 - INFO - root -   train_loss after step 8850: 0.34900449216365814\n",
            "12/01/2019 08:11:36 - INFO - root -   lr after step 8900: 5.160731064576388e-05\n",
            "12/01/2019 08:11:36 - INFO - root -   train_loss after step 8900: 0.37531202405691144\n",
            "12/01/2019 08:12:04 - INFO - root -   lr after step 8950: 5.151211522743897e-05\n",
            "12/01/2019 08:12:04 - INFO - root -   train_loss after step 8950: 0.3853382551670074\n",
            "12/01/2019 08:12:32 - INFO - root -   lr after step 9000: 5.141647183809849e-05\n",
            "12/01/2019 08:12:32 - INFO - root -   train_loss after step 9000: 0.36931540340185165\n",
            "12/01/2019 08:13:00 - INFO - root -   lr after step 9050: 5.132038246943254e-05\n",
            "12/01/2019 08:13:00 - INFO - root -   train_loss after step 9050: 0.35738900274038315\n",
            "12/01/2019 08:13:29 - INFO - root -   lr after step 9100: 5.1223849122418294e-05\n",
            "12/01/2019 08:13:29 - INFO - root -   train_loss after step 9100: 0.3917330029606819\n",
            "12/01/2019 08:13:57 - INFO - root -   lr after step 9150: 5.1126873807278424e-05\n",
            "12/01/2019 08:13:57 - INFO - root -   train_loss after step 9150: 0.40033585131168364\n",
            "12/01/2019 08:14:25 - INFO - root -   lr after step 9200: 5.1029458543439204e-05\n",
            "12/01/2019 08:14:25 - INFO - root -   train_loss after step 9200: 0.3574613210558891\n",
            "12/01/2019 08:14:53 - INFO - root -   lr after step 9250: 5.093160535948841e-05\n",
            "12/01/2019 08:14:53 - INFO - root -   train_loss after step 9250: 0.36553787261247633\n",
            "12/01/2019 08:15:22 - INFO - root -   lr after step 9300: 5.083331629313319e-05\n",
            "12/01/2019 08:15:22 - INFO - root -   train_loss after step 9300: 0.40432930380105975\n",
            "12/01/2019 08:15:50 - INFO - root -   lr after step 9350: 5.0734593391157506e-05\n",
            "12/01/2019 08:15:50 - INFO - root -   train_loss after step 9350: 0.37717361718416215\n",
            "12/01/2019 08:16:18 - INFO - root -   lr after step 9400: 5.063543870937959e-05\n",
            "12/01/2019 08:16:18 - INFO - root -   train_loss after step 9400: 0.3589798179268837\n",
            "12/01/2019 08:16:46 - INFO - root -   lr after step 9450: 5.053585431260911e-05\n",
            "12/01/2019 08:16:46 - INFO - root -   train_loss after step 9450: 0.3645658060908318\n",
            "12/01/2019 08:17:14 - INFO - root -   lr after step 9500: 5.043584227460416e-05\n",
            "12/01/2019 08:17:14 - INFO - root -   train_loss after step 9500: 0.39452870428562165\n",
            "12/01/2019 08:17:43 - INFO - root -   lr after step 9550: 5.033540467802813e-05\n",
            "12/01/2019 08:17:43 - INFO - root -   train_loss after step 9550: 0.38208327800035474\n",
            "12/01/2019 08:18:11 - INFO - root -   lr after step 9600: 5.0234543614406264e-05\n",
            "12/01/2019 08:18:11 - INFO - root -   train_loss after step 9600: 0.3610779657959938\n",
            "12/01/2019 08:18:39 - INFO - root -   lr after step 9650: 5.013326118408212e-05\n",
            "12/01/2019 08:18:39 - INFO - root -   train_loss after step 9650: 0.3788415268063545\n",
            "12/01/2019 08:19:07 - INFO - root -   lr after step 9700: 5.003155949617389e-05\n",
            "12/01/2019 08:19:07 - INFO - root -   train_loss after step 9700: 0.376063112616539\n",
            "12/01/2019 08:19:35 - INFO - root -   lr after step 9750: 4.99294406685304e-05\n",
            "12/01/2019 08:19:35 - INFO - root -   train_loss after step 9750: 0.3756951114535332\n",
            "12/01/2019 08:20:04 - INFO - root -   lr after step 9800: 4.982690682768709e-05\n",
            "12/01/2019 08:20:04 - INFO - root -   train_loss after step 9800: 0.4041276925802231\n",
            "12/01/2019 08:20:32 - INFO - root -   lr after step 9850: 4.972396010882166e-05\n",
            "12/01/2019 08:20:32 - INFO - root -   train_loss after step 9850: 0.3694871956110001\n",
            "12/01/2019 08:21:00 - INFO - root -   lr after step 9900: 4.962060265570963e-05\n",
            "12/01/2019 08:21:00 - INFO - root -   train_loss after step 9900: 0.37030541568994524\n",
            "12/01/2019 08:21:28 - INFO - root -   lr after step 9950: 4.9516836620679715e-05\n",
            "12/01/2019 08:21:28 - INFO - root -   train_loss after step 9950: 0.3717454564571381\n",
            "12/01/2019 08:21:56 - INFO - root -   lr after step 10000: 4.941266416456898e-05\n",
            "12/01/2019 08:21:56 - INFO - root -   train_loss after step 10000: 0.37083551675081255\n",
            "12/01/2019 08:22:25 - INFO - root -   lr after step 10050: 4.930808745667788e-05\n",
            "12/01/2019 08:22:25 - INFO - root -   train_loss after step 10050: 0.3928552556037903\n",
            "12/01/2019 08:22:53 - INFO - root -   lr after step 10100: 4.920310867472502e-05\n",
            "12/01/2019 08:22:53 - INFO - root -   train_loss after step 10100: 0.4053207093477249\n",
            "12/01/2019 08:23:21 - INFO - root -   lr after step 10150: 4.909773000480188e-05\n",
            "12/01/2019 08:23:21 - INFO - root -   train_loss after step 10150: 0.37332223504781725\n",
            "12/01/2019 08:23:49 - INFO - root -   lr after step 10200: 4.8991953641327255e-05\n",
            "12/01/2019 08:23:49 - INFO - root -   train_loss after step 10200: 0.39479892879724504\n",
            "12/01/2019 08:24:17 - INFO - root -   lr after step 10250: 4.888578178700154e-05\n",
            "12/01/2019 08:24:17 - INFO - root -   train_loss after step 10250: 0.38331899017095566\n",
            "12/01/2019 08:24:45 - INFO - root -   lr after step 10300: 4.877921665276089e-05\n",
            "12/01/2019 08:24:45 - INFO - root -   train_loss after step 10300: 0.35933149337768555\n",
            "12/01/2019 08:25:13 - INFO - root -   lr after step 10350: 4.867226045773118e-05\n",
            "12/01/2019 08:25:13 - INFO - root -   train_loss after step 10350: 0.377580001950264\n",
            "12/01/2019 08:25:41 - INFO - root -   lr after step 10400: 4.856491542918178e-05\n",
            "12/01/2019 08:25:41 - INFO - root -   train_loss after step 10400: 0.37305106073617933\n",
            "12/01/2019 08:26:10 - INFO - root -   lr after step 10450: 4.8457183802479185e-05\n",
            "12/01/2019 08:26:10 - INFO - root -   train_loss after step 10450: 0.3757853239774704\n",
            "12/01/2019 08:26:38 - INFO - root -   lr after step 10500: 4.834906782104044e-05\n",
            "12/01/2019 08:26:38 - INFO - root -   train_loss after step 10500: 0.3529183992743492\n",
            "12/01/2019 08:27:06 - INFO - root -   lr after step 10550: 4.824056973628645e-05\n",
            "12/01/2019 08:27:06 - INFO - root -   train_loss after step 10550: 0.39533057987689973\n",
            "12/01/2019 08:27:34 - INFO - root -   lr after step 10600: 4.81316918075951e-05\n",
            "12/01/2019 08:27:34 - INFO - root -   train_loss after step 10600: 0.3570766413211823\n",
            "12/01/2019 08:28:02 - INFO - root -   lr after step 10650: 4.802243630225418e-05\n",
            "12/01/2019 08:28:02 - INFO - root -   train_loss after step 10650: 0.34808496475219725\n",
            "12/01/2019 08:28:30 - INFO - root -   lr after step 10700: 4.7912805495414196e-05\n",
            "12/01/2019 08:28:30 - INFO - root -   train_loss after step 10700: 0.40237254738807676\n",
            "12/01/2019 08:28:59 - INFO - root -   lr after step 10750: 4.7802801670040966e-05\n",
            "12/01/2019 08:28:59 - INFO - root -   train_loss after step 10750: 0.36585613787174226\n",
            "12/01/2019 08:29:27 - INFO - root -   lr after step 10800: 4.7692427116868095e-05\n",
            "12/01/2019 08:29:27 - INFO - root -   train_loss after step 10800: 0.37682354122400286\n",
            "12/01/2019 08:29:55 - INFO - root -   lr after step 10850: 4.758168413434928e-05\n",
            "12/01/2019 08:29:55 - INFO - root -   train_loss after step 10850: 0.38232145011425017\n",
            "12/01/2019 08:30:23 - INFO - root -   lr after step 10900: 4.7470575028610446e-05\n",
            "12/01/2019 08:30:23 - INFO - root -   train_loss after step 10900: 0.37661713659763335\n",
            "12/01/2019 08:30:51 - INFO - root -   lr after step 10950: 4.7359102113401676e-05\n",
            "12/01/2019 08:30:51 - INFO - root -   train_loss after step 10950: 0.3817113599181175\n",
            "12/01/2019 08:31:19 - INFO - root -   lr after step 11000: 4.72472677100491e-05\n",
            "12/01/2019 08:31:19 - INFO - root -   train_loss after step 11000: 0.3655752170085907\n",
            "12/01/2019 08:31:48 - INFO - root -   lr after step 11050: 4.713507414740654e-05\n",
            "12/01/2019 08:31:48 - INFO - root -   train_loss after step 11050: 0.3552844896912575\n",
            "12/01/2019 08:32:16 - INFO - root -   lr after step 11100: 4.702252376180694e-05\n",
            "12/01/2019 08:32:16 - INFO - root -   train_loss after step 11100: 0.36771881133317946\n",
            "12/01/2019 08:32:44 - INFO - root -   lr after step 11150: 4.690961889701382e-05\n",
            "12/01/2019 08:32:44 - INFO - root -   train_loss after step 11150: 0.3699346843361855\n",
            "12/01/2019 08:33:12 - INFO - root -   lr after step 11200: 4.67963619041724e-05\n",
            "12/01/2019 08:33:12 - INFO - root -   train_loss after step 11200: 0.3822773540019989\n",
            "12/01/2019 08:33:41 - INFO - root -   lr after step 11250: 4.668275514176066e-05\n",
            "12/01/2019 08:33:41 - INFO - root -   train_loss after step 11250: 0.37928229063749314\n",
            "12/01/2019 08:34:09 - INFO - root -   lr after step 11300: 4.6568800975540226e-05\n",
            "12/01/2019 08:34:09 - INFO - root -   train_loss after step 11300: 0.36078222513198854\n",
            "12/01/2019 08:34:37 - INFO - root -   lr after step 11350: 4.6454501778507104e-05\n",
            "12/01/2019 08:34:37 - INFO - root -   train_loss after step 11350: 0.37431786954402924\n",
            "12/01/2019 08:35:05 - INFO - root -   lr after step 11400: 4.6339859930842285e-05\n",
            "12/01/2019 08:35:05 - INFO - root -   train_loss after step 11400: 0.3496936032176018\n",
            "12/01/2019 08:35:33 - INFO - root -   lr after step 11450: 4.622487781986212e-05\n",
            "12/01/2019 08:35:33 - INFO - root -   train_loss after step 11450: 0.3879115492105484\n",
            "12/01/2019 08:36:02 - INFO - root -   lr after step 11500: 4.610955783996868e-05\n",
            "12/01/2019 08:36:02 - INFO - root -   train_loss after step 11500: 0.37116641610860823\n",
            "12/01/2019 08:36:30 - INFO - root -   lr after step 11550: 4.599390239259985e-05\n",
            "12/01/2019 08:36:30 - INFO - root -   train_loss after step 11550: 0.3592099890112877\n",
            "12/01/2019 08:36:58 - INFO - root -   lr after step 11600: 4.587791388617934e-05\n",
            "12/01/2019 08:36:58 - INFO - root -   train_loss after step 11600: 0.3750291728973389\n",
            "12/01/2019 08:37:26 - INFO - root -   lr after step 11650: 4.5761594736066504e-05\n",
            "12/01/2019 08:37:26 - INFO - root -   train_loss after step 11650: 0.36903891652822496\n",
            "12/01/2019 08:37:54 - INFO - root -   lr after step 11700: 4.564494736450608e-05\n",
            "12/01/2019 08:37:54 - INFO - root -   train_loss after step 11700: 0.3636432677507401\n",
            "12/01/2019 08:38:22 - INFO - root -   lr after step 11750: 4.552797420057773e-05\n",
            "12/01/2019 08:38:22 - INFO - root -   train_loss after step 11750: 0.3898418709635735\n",
            "12/01/2019 08:38:51 - INFO - root -   lr after step 11800: 4.5410677680145447e-05\n",
            "12/01/2019 08:38:51 - INFO - root -   train_loss after step 11800: 0.38048044860363006\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 08:39:19 - INFO - root -   lr after step 11850: 4.529306024580685e-05\n",
            "12/01/2019 08:39:19 - INFO - root -   train_loss after step 11850: 0.37356269896030425\n",
            "12/01/2019 08:39:48 - INFO - root -   lr after step 11900: 4.517512434684231e-05\n",
            "12/01/2019 08:39:48 - INFO - root -   train_loss after step 11900: 0.3705934026837349\n",
            "12/01/2019 08:40:16 - INFO - root -   lr after step 11950: 4.505687243916394e-05\n",
            "12/01/2019 08:40:16 - INFO - root -   train_loss after step 11950: 0.3439816263318062\n",
            "12/01/2019 08:40:45 - INFO - root -   lr after step 12000: 4.493830698526447e-05\n",
            "12/01/2019 08:40:45 - INFO - root -   train_loss after step 12000: 0.36968630850315093\n",
            "12/01/2019 08:41:13 - INFO - root -   lr after step 12050: 4.4819430454165936e-05\n",
            "12/01/2019 08:41:13 - INFO - root -   train_loss after step 12050: 0.36934063613414764\n",
            "12/01/2019 08:41:41 - INFO - root -   lr after step 12100: 4.470024532136829e-05\n",
            "12/01/2019 08:41:41 - INFO - root -   train_loss after step 12100: 0.3727448379993439\n",
            "12/01/2019 08:42:09 - INFO - root -   lr after step 12150: 4.458075406879788e-05\n",
            "12/01/2019 08:42:09 - INFO - root -   train_loss after step 12150: 0.3514365193247795\n",
            "12/01/2019 08:42:37 - INFO - root -   lr after step 12200: 4.4460959184755675e-05\n",
            "12/01/2019 08:42:37 - INFO - root -   train_loss after step 12200: 0.380926171541214\n",
            "12/01/2019 08:43:06 - INFO - root -   lr after step 12250: 4.434086316386555e-05\n",
            "12/01/2019 08:43:06 - INFO - root -   train_loss after step 12250: 0.3899431550502777\n",
            "12/01/2019 08:43:34 - INFO - root -   lr after step 12300: 4.422046850702227e-05\n",
            "12/01/2019 08:43:34 - INFO - root -   train_loss after step 12300: 0.38354895800352096\n",
            "12/01/2019 08:44:02 - INFO - root -   lr after step 12350: 4.4099777721339415e-05\n",
            "12/01/2019 08:44:02 - INFO - root -   train_loss after step 12350: 0.36754672944545747\n",
            "12/01/2019 08:44:30 - INFO - root -   lr after step 12400: 4.397879332009724e-05\n",
            "12/01/2019 08:44:30 - INFO - root -   train_loss after step 12400: 0.3869558584690094\n",
            "12/01/2019 08:44:59 - INFO - root -   lr after step 12450: 4.3857517822690255e-05\n",
            "12/01/2019 08:44:59 - INFO - root -   train_loss after step 12450: 0.35385751485824585\n",
            "12/01/2019 08:45:27 - INFO - root -   lr after step 12500: 4.373595375457479e-05\n",
            "12/01/2019 08:45:27 - INFO - root -   train_loss after step 12500: 0.3742705485224724\n",
            "12/01/2019 08:45:55 - INFO - root -   lr after step 12550: 4.361410364721642e-05\n",
            "12/01/2019 08:45:55 - INFO - root -   train_loss after step 12550: 0.37173533618450166\n",
            "12/01/2019 08:46:24 - INFO - root -   lr after step 12600: 4.349197003803722e-05\n",
            "12/01/2019 08:46:24 - INFO - root -   train_loss after step 12600: 0.3661775287985802\n",
            "12/01/2019 08:46:52 - INFO - root -   lr after step 12650: 4.3369555470362975e-05\n",
            "12/01/2019 08:46:52 - INFO - root -   train_loss after step 12650: 0.3907555386424065\n",
            "12/01/2019 08:47:20 - INFO - root -   lr after step 12700: 4.324686249337016e-05\n",
            "12/01/2019 08:47:20 - INFO - root -   train_loss after step 12700: 0.3672044140100479\n",
            "12/01/2019 08:47:48 - INFO - root -   lr after step 12750: 4.3123893662032884e-05\n",
            "12/01/2019 08:47:48 - INFO - root -   train_loss after step 12750: 0.38166471749544145\n",
            "12/01/2019 08:48:17 - INFO - root -   lr after step 12800: 4.3000651537069686e-05\n",
            "12/01/2019 08:48:17 - INFO - root -   train_loss after step 12800: 0.3539854621887207\n",
            "12/01/2019 08:48:45 - INFO - root -   lr after step 12850: 4.28771386848902e-05\n",
            "12/01/2019 08:48:45 - INFO - root -   train_loss after step 12850: 0.37729301422834394\n",
            "12/01/2019 08:49:13 - INFO - root -   lr after step 12900: 4.2753357677541734e-05\n",
            "12/01/2019 08:49:13 - INFO - root -   train_loss after step 12900: 0.3684834033250809\n",
            "12/01/2019 08:49:41 - INFO - root -   lr after step 12950: 4.2629311092655666e-05\n",
            "12/01/2019 08:49:41 - INFO - root -   train_loss after step 12950: 0.3710251849889755\n",
            "12/01/2019 08:50:10 - INFO - root -   lr after step 13000: 4.250500151339383e-05\n",
            "12/01/2019 08:50:10 - INFO - root -   train_loss after step 13000: 0.3796926227211952\n",
            "12/01/2019 08:50:38 - INFO - root -   lr after step 13050: 4.238043152839465e-05\n",
            "12/01/2019 08:50:38 - INFO - root -   train_loss after step 13050: 0.3621268290281296\n",
            "12/01/2019 08:51:07 - INFO - root -   lr after step 13100: 4.2255603731719316e-05\n",
            "12/01/2019 08:51:07 - INFO - root -   train_loss after step 13100: 0.34283825010061264\n",
            "12/01/2019 08:51:35 - INFO - root -   lr after step 13150: 4.213052072279767e-05\n",
            "12/01/2019 08:51:35 - INFO - root -   train_loss after step 13150: 0.33929848432540893\n",
            "12/01/2019 08:52:03 - INFO - root -   lr after step 13200: 4.2005185106374166e-05\n",
            "12/01/2019 08:52:03 - INFO - root -   train_loss after step 13200: 0.3812769281864166\n",
            "12/01/2019 08:52:31 - INFO - root -   lr after step 13250: 4.187959949245357e-05\n",
            "12/01/2019 08:52:31 - INFO - root -   train_loss after step 13250: 0.37521884351968765\n",
            "12/01/2019 08:53:00 - INFO - root -   lr after step 13300: 4.175376649624664e-05\n",
            "12/01/2019 08:53:00 - INFO - root -   train_loss after step 13300: 0.3826551797986031\n",
            "12/01/2019 08:53:28 - INFO - root -   lr after step 13350: 4.162768873811564e-05\n",
            "12/01/2019 08:53:28 - INFO - root -   train_loss after step 13350: 0.35281520932912824\n",
            "12/01/2019 08:53:56 - INFO - root -   lr after step 13400: 4.150136884351979e-05\n",
            "12/01/2019 08:53:56 - INFO - root -   train_loss after step 13400: 0.35256376713514326\n",
            "12/01/2019 08:54:25 - INFO - root -   lr after step 13450: 4.137480944296059e-05\n",
            "12/01/2019 08:54:25 - INFO - root -   train_loss after step 13450: 0.37056097000837324\n",
            "12/01/2019 08:54:53 - INFO - root -   lr after step 13500: 4.124801317192707e-05\n",
            "12/01/2019 08:54:53 - INFO - root -   train_loss after step 13500: 0.38486621528863907\n",
            "12/01/2019 08:55:21 - INFO - root -   lr after step 13550: 4.1120982670840844e-05\n",
            "12/01/2019 08:55:21 - INFO - root -   train_loss after step 13550: 0.3856602743268013\n",
            "12/01/2019 08:55:50 - INFO - root -   lr after step 13600: 4.0993720585001165e-05\n",
            "12/01/2019 08:55:50 - INFO - root -   train_loss after step 13600: 0.3558261153101921\n",
            "12/01/2019 08:56:18 - INFO - root -   lr after step 13650: 4.0866229564529866e-05\n",
            "12/01/2019 08:56:18 - INFO - root -   train_loss after step 13650: 0.3620974498987198\n",
            "12/01/2019 08:56:46 - INFO - root -   lr after step 13700: 4.073851226431612e-05\n",
            "12/01/2019 08:56:46 - INFO - root -   train_loss after step 13700: 0.3640092912316322\n",
            "12/01/2019 08:57:15 - INFO - root -   lr after step 13750: 4.061057134396116e-05\n",
            "12/01/2019 08:57:15 - INFO - root -   train_loss after step 13750: 0.3698661741614342\n",
            "12/01/2019 08:57:43 - INFO - root -   lr after step 13800: 4.0482409467722976e-05\n",
            "12/01/2019 08:57:43 - INFO - root -   train_loss after step 13800: 0.3462705031037331\n",
            "12/01/2019 08:58:12 - INFO - root -   lr after step 13850: 4.03540293044607e-05\n",
            "12/01/2019 08:58:12 - INFO - root -   train_loss after step 13850: 0.3484476390480995\n",
            "12/01/2019 08:58:40 - INFO - root -   lr after step 13900: 4.022543352757914e-05\n",
            "12/01/2019 08:58:40 - INFO - root -   train_loss after step 13900: 0.3517432706058025\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 08:59:08 - INFO - root -   lr after step 13950: 4.009662481497308e-05\n",
            "12/01/2019 08:59:08 - INFO - root -   train_loss after step 13950: 0.3754798698425293\n",
            "12/01/2019 08:59:36 - INFO - root -   lr after step 14000: 3.996760584897146e-05\n",
            "12/01/2019 08:59:36 - INFO - root -   train_loss after step 14000: 0.3617463579773903\n",
            "12/01/2019 09:00:05 - INFO - root -   lr after step 14050: 3.9838379316281605e-05\n",
            "12/01/2019 09:00:05 - INFO - root -   train_loss after step 14050: 0.36466491252183914\n",
            "12/01/2019 09:00:33 - INFO - root -   lr after step 14100: 3.970894790793322e-05\n",
            "12/01/2019 09:00:33 - INFO - root -   train_loss after step 14100: 0.3770527705550194\n",
            "12/01/2019 09:01:01 - INFO - root -   lr after step 14150: 3.957931431922236e-05\n",
            "12/01/2019 09:01:01 - INFO - root -   train_loss after step 14150: 0.3642402628064156\n",
            "12/01/2019 09:01:30 - INFO - root -   lr after step 14200: 3.944948124965531e-05\n",
            "12/01/2019 09:01:30 - INFO - root -   train_loss after step 14200: 0.3689429911971092\n",
            "12/01/2019 09:01:58 - INFO - root -   lr after step 14250: 3.9319451402892395e-05\n",
            "12/01/2019 09:01:58 - INFO - root -   train_loss after step 14250: 0.38654893308877947\n",
            "12/01/2019 09:02:26 - INFO - root -   lr after step 14300: 3.9189227486691614e-05\n",
            "12/01/2019 09:02:26 - INFO - root -   train_loss after step 14300: 0.3447985741496086\n",
            "12/01/2019 09:02:54 - INFO - root -   lr after step 14350: 3.9058812212852325e-05\n",
            "12/01/2019 09:02:54 - INFO - root -   train_loss after step 14350: 0.3762529823184013\n",
            "12/01/2019 09:03:23 - INFO - root -   lr after step 14400: 3.892820829715872e-05\n",
            "12/01/2019 09:03:23 - INFO - root -   train_loss after step 14400: 0.3651956474781036\n",
            "12/01/2019 09:03:51 - INFO - root -   lr after step 14450: 3.8797418459323306e-05\n",
            "12/01/2019 09:03:51 - INFO - root -   train_loss after step 14450: 0.3766623678803444\n",
            "12/01/2019 09:04:19 - INFO - root -   lr after step 14500: 3.8666445422930255e-05\n",
            "12/01/2019 09:04:19 - INFO - root -   train_loss after step 14500: 0.3620091912150383\n",
            "12/01/2019 09:04:47 - INFO - root -   lr after step 14550: 3.8535291915378686e-05\n",
            "12/01/2019 09:04:47 - INFO - root -   train_loss after step 14550: 0.361305350959301\n",
            "12/01/2019 09:05:15 - INFO - root -   lr after step 14600: 3.8403960667825866e-05\n",
            "12/01/2019 09:05:15 - INFO - root -   train_loss after step 14600: 0.34477164059877397\n",
            "12/01/2019 09:05:44 - INFO - root -   lr after step 14650: 3.827245441513035e-05\n",
            "12/01/2019 09:05:44 - INFO - root -   train_loss after step 14650: 0.34233109146356583\n",
            "12/01/2019 09:06:12 - INFO - root -   lr after step 14700: 3.8140775895795025e-05\n",
            "12/01/2019 09:06:12 - INFO - root -   train_loss after step 14700: 0.3525411193072796\n",
            "12/01/2019 09:06:40 - INFO - root -   lr after step 14750: 3.800892785191007e-05\n",
            "12/01/2019 09:06:40 - INFO - root -   train_loss after step 14750: 0.36691092789173124\n",
            "12/01/2019 09:07:08 - INFO - root -   lr after step 14800: 3.7876913029095885e-05\n",
            "12/01/2019 09:07:08 - INFO - root -   train_loss after step 14800: 0.3812384960055351\n",
            "12/01/2019 09:07:37 - INFO - root -   lr after step 14850: 3.774473417644587e-05\n",
            "12/01/2019 09:07:37 - INFO - root -   train_loss after step 14850: 0.3722072795033455\n",
            "12/01/2019 09:08:05 - INFO - root -   lr after step 14900: 3.761239404646921e-05\n",
            "12/01/2019 09:08:05 - INFO - root -   train_loss after step 14900: 0.3600639489293098\n",
            "12/01/2019 09:08:33 - INFO - root -   lr after step 14950: 3.7479895395033574e-05\n",
            "12/01/2019 09:08:33 - INFO - root -   train_loss after step 14950: 0.3652400666475296\n",
            "12/01/2019 09:09:01 - INFO - root -   lr after step 15000: 3.734724098130767e-05\n",
            "12/01/2019 09:09:01 - INFO - root -   train_loss after step 15000: 0.3258530583977699\n",
            "12/01/2019 09:09:30 - INFO - root -   lr after step 15050: 3.7214433567703834e-05\n",
            "12/01/2019 09:09:30 - INFO - root -   train_loss after step 15050: 0.35923742115497587\n",
            "12/01/2019 09:09:58 - INFO - root -   lr after step 15100: 3.708147591982047e-05\n",
            "12/01/2019 09:09:58 - INFO - root -   train_loss after step 15100: 0.3462872821092606\n",
            "12/01/2019 09:10:26 - INFO - root -   lr after step 15150: 3.694837080638454e-05\n",
            "12/01/2019 09:10:26 - INFO - root -   train_loss after step 15150: 0.36908434391021727\n",
            "12/01/2019 09:10:54 - INFO - root -   lr after step 15200: 3.681512099919377e-05\n",
            "12/01/2019 09:10:54 - INFO - root -   train_loss after step 15200: 0.3610029733181\n",
            "12/01/2019 09:11:22 - INFO - root -   lr after step 15250: 3.6681729273059045e-05\n",
            "12/01/2019 09:11:22 - INFO - root -   train_loss after step 15250: 0.34920038521289826\n",
            "12/01/2019 09:11:50 - INFO - root -   lr after step 15300: 3.65481984057466e-05\n",
            "12/01/2019 09:11:50 - INFO - root -   train_loss after step 15300: 0.37943376511335375\n",
            "12/01/2019 09:12:18 - INFO - root -   lr after step 15350: 3.641453117792016e-05\n",
            "12/01/2019 09:12:18 - INFO - root -   train_loss after step 15350: 0.3750867927074432\n",
            "12/01/2019 09:12:47 - INFO - root -   lr after step 15400: 3.6280730373083e-05\n",
            "12/01/2019 09:12:47 - INFO - root -   train_loss after step 15400: 0.3739265477657318\n",
            "12/01/2019 09:13:15 - INFO - root -   lr after step 15450: 3.614679877752006e-05\n",
            "12/01/2019 09:13:15 - INFO - root -   train_loss after step 15450: 0.3800974291563034\n",
            "12/01/2019 09:13:43 - INFO - root -   lr after step 15500: 3.6012739180239885e-05\n",
            "12/01/2019 09:13:43 - INFO - root -   train_loss after step 15500: 0.3589512088894844\n",
            "12/01/2019 09:14:11 - INFO - root -   lr after step 15550: 3.587855437291651e-05\n",
            "12/01/2019 09:14:11 - INFO - root -   train_loss after step 15550: 0.3288845267891884\n",
            "12/01/2019 09:14:39 - INFO - root -   lr after step 15600: 3.574424714983138e-05\n",
            "12/01/2019 09:14:39 - INFO - root -   train_loss after step 15600: 0.3682914820313454\n",
            "12/01/2019 09:15:07 - INFO - root -   lr after step 15650: 3.560982030781514e-05\n",
            "12/01/2019 09:15:07 - INFO - root -   train_loss after step 15650: 0.3890742814540863\n",
            "12/01/2019 09:15:35 - INFO - root -   lr after step 15700: 3.547527664618942e-05\n",
            "12/01/2019 09:15:35 - INFO - root -   train_loss after step 15700: 0.37891668945550916\n",
            "12/01/2019 09:16:04 - INFO - root -   lr after step 15750: 3.5340618966708465e-05\n",
            "12/01/2019 09:16:04 - INFO - root -   train_loss after step 15750: 0.3530931782722473\n",
            "12/01/2019 09:16:32 - INFO - root -   lr after step 15800: 3.5205850073500875e-05\n",
            "12/01/2019 09:16:32 - INFO - root -   train_loss after step 15800: 0.3663040339946747\n",
            "12/01/2019 09:17:00 - INFO - root -   lr after step 15850: 3.5070972773011185e-05\n",
            "12/01/2019 09:17:00 - INFO - root -   train_loss after step 15850: 0.3721170499920845\n",
            "12/01/2019 09:17:28 - INFO - root -   lr after step 15900: 3.4935989873941384e-05\n",
            "12/01/2019 09:17:28 - INFO - root -   train_loss after step 15900: 0.359021977186203\n",
            "12/01/2019 09:17:57 - INFO - root -   lr after step 15950: 3.4800904187192504e-05\n",
            "12/01/2019 09:17:57 - INFO - root -   train_loss after step 15950: 0.38258258074522017\n",
            "12/01/2019 09:18:25 - INFO - root -   lr after step 16000: 3.4665718525805994e-05\n",
            "12/01/2019 09:18:25 - INFO - root -   train_loss after step 16000: 0.339415944814682\n",
            "12/01/2019 09:18:54 - INFO - root -   lr after step 16050: 3.453043570490522e-05\n",
            "12/01/2019 09:18:54 - INFO - root -   train_loss after step 16050: 0.3681924068927765\n",
            "12/01/2019 09:19:22 - INFO - root -   lr after step 16100: 3.439505854163681e-05\n",
            "12/01/2019 09:19:22 - INFO - root -   train_loss after step 16100: 0.35950309067964553\n",
            "12/01/2019 09:19:50 - INFO - root -   lr after step 16150: 3.425958985511196e-05\n",
            "12/01/2019 09:19:50 - INFO - root -   train_loss after step 16150: 0.35235619693994524\n",
            "12/01/2019 09:20:18 - INFO - root -   lr after step 16200: 3.412403246634779e-05\n",
            "12/01/2019 09:20:18 - INFO - root -   train_loss after step 16200: 0.3589866051077843\n",
            "12/01/2019 09:20:47 - INFO - root -   lr after step 16250: 3.398838919820856e-05\n",
            "12/01/2019 09:20:47 - INFO - root -   train_loss after step 16250: 0.342387338578701\n",
            "12/01/2019 09:21:15 - INFO - root -   lr after step 16300: 3.385266287534688e-05\n",
            "12/01/2019 09:21:15 - INFO - root -   train_loss after step 16300: 0.33708896160125734\n",
            "12/01/2019 09:21:43 - INFO - root -   lr after step 16350: 3.37168563241449e-05\n",
            "12/01/2019 09:21:43 - INFO - root -   train_loss after step 16350: 0.33618766397237776\n",
            "12/01/2019 09:22:11 - INFO - root -   lr after step 16400: 3.358097237265547e-05\n",
            "12/01/2019 09:22:11 - INFO - root -   train_loss after step 16400: 0.37614179253578184\n",
            "12/01/2019 09:22:40 - INFO - root -   lr after step 16450: 3.344501385054324e-05\n",
            "12/01/2019 09:22:40 - INFO - root -   train_loss after step 16450: 0.35191270291805266\n",
            "12/01/2019 09:23:08 - INFO - root -   lr after step 16500: 3.3308983589025704e-05\n",
            "12/01/2019 09:23:08 - INFO - root -   train_loss after step 16500: 0.34008459001779556\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 09:23:36 - INFO - root -   lr after step 16550: 3.317288442081429e-05\n",
            "12/01/2019 09:23:36 - INFO - root -   train_loss after step 16550: 0.402147488296032\n",
            "12/01/2019 09:24:04 - INFO - root -   lr after step 16600: 3.303671918005532e-05\n",
            "12/01/2019 09:24:04 - INFO - root -   train_loss after step 16600: 0.34062888264656066\n",
            "12/01/2019 09:24:32 - INFO - root -   lr after step 16650: 3.290049070227105e-05\n",
            "12/01/2019 09:24:32 - INFO - root -   train_loss after step 16650: 0.36112097173929214\n",
            "12/01/2019 09:25:01 - INFO - root -   lr after step 16700: 3.2764201824300565e-05\n",
            "12/01/2019 09:25:01 - INFO - root -   train_loss after step 16700: 0.36374580055475236\n",
            "12/01/2019 09:25:29 - INFO - root -   lr after step 16750: 3.262785538424076e-05\n",
            "12/01/2019 09:25:29 - INFO - root -   train_loss after step 16750: 0.35450701028108594\n",
            "12/01/2019 09:25:57 - INFO - root -   lr after step 16800: 3.249145422138717e-05\n",
            "12/01/2019 09:25:57 - INFO - root -   train_loss after step 16800: 0.3730134156346321\n",
            "12/01/2019 09:26:25 - INFO - root -   lr after step 16850: 3.235500117617493e-05\n",
            "12/01/2019 09:26:25 - INFO - root -   train_loss after step 16850: 0.3491781643033028\n",
            "12/01/2019 09:26:53 - INFO - root -   lr after step 16900: 3.221849909011954e-05\n",
            "12/01/2019 09:26:53 - INFO - root -   train_loss after step 16900: 0.3783344781398773\n",
            "12/01/2019 09:27:22 - INFO - root -   lr after step 16950: 3.2081950805757755e-05\n",
            "12/01/2019 09:27:22 - INFO - root -   train_loss after step 16950: 0.3747056290507317\n",
            "12/01/2019 09:27:50 - INFO - root -   lr after step 17000: 3.194535916658838e-05\n",
            "12/01/2019 09:27:50 - INFO - root -   train_loss after step 17000: 0.3585419845581055\n",
            "12/01/2019 09:28:18 - INFO - root -   lr after step 17050: 3.1808727017013016e-05\n",
            "12/01/2019 09:28:18 - INFO - root -   train_loss after step 17050: 0.3395045161247253\n",
            "12/01/2019 09:28:46 - INFO - root -   lr after step 17100: 3.1672057202276876e-05\n",
            "12/01/2019 09:28:46 - INFO - root -   train_loss after step 17100: 0.3844438397884369\n",
            "12/01/2019 09:29:14 - INFO - root -   lr after step 17150: 3.1535352568409515e-05\n",
            "12/01/2019 09:29:14 - INFO - root -   train_loss after step 17150: 0.3607371711730957\n",
            "12/01/2019 09:29:42 - INFO - root -   lr after step 17200: 3.139861596216556e-05\n",
            "12/01/2019 09:29:42 - INFO - root -   train_loss after step 17200: 0.38676551938056947\n",
            "12/01/2019 09:30:10 - INFO - root -   lr after step 17250: 3.126185023096545e-05\n",
            "12/01/2019 09:30:10 - INFO - root -   train_loss after step 17250: 0.33748670786619184\n",
            "12/01/2019 09:30:39 - INFO - root -   lr after step 17300: 3.1125058222836096e-05\n",
            "12/01/2019 09:30:39 - INFO - root -   train_loss after step 17300: 0.36489501416683195\n",
            "12/01/2019 09:31:07 - INFO - root -   lr after step 17350: 3.0988242786351647e-05\n",
            "12/01/2019 09:31:07 - INFO - root -   train_loss after step 17350: 0.35089008152484896\n",
            "12/01/2019 09:31:35 - INFO - root -   lr after step 17400: 3.085140677057409e-05\n",
            "12/01/2019 09:31:35 - INFO - root -   train_loss after step 17400: 0.36242382749915125\n",
            "12/01/2019 09:32:03 - INFO - root -   lr after step 17450: 3.0714553024993974e-05\n",
            "12/01/2019 09:32:03 - INFO - root -   train_loss after step 17450: 0.3684009334445\n",
            "12/01/2019 09:32:31 - INFO - root -   lr after step 17500: 3.057768439947105e-05\n",
            "12/01/2019 09:32:31 - INFO - root -   train_loss after step 17500: 0.36162809818983077\n",
            "12/01/2019 09:32:59 - INFO - root -   lr after step 17550: 3.0440803744174928e-05\n",
            "12/01/2019 09:32:59 - INFO - root -   train_loss after step 17550: 0.3574733391404152\n",
            "12/01/2019 09:33:28 - INFO - root -   lr after step 17600: 3.030391390952574e-05\n",
            "12/01/2019 09:33:28 - INFO - root -   train_loss after step 17600: 0.3435042768716812\n",
            "12/01/2019 09:33:56 - INFO - root -   lr after step 17650: 3.016701774613475e-05\n",
            "12/01/2019 09:33:56 - INFO - root -   train_loss after step 17650: 0.3644708859920502\n",
            "12/01/2019 09:34:24 - INFO - root -   lr after step 17700: 3.0030118104745048e-05\n",
            "12/01/2019 09:34:24 - INFO - root -   train_loss after step 17700: 0.393847676217556\n",
            "12/01/2019 09:34:52 - INFO - root -   lr after step 17750: 2.98932178361721e-05\n",
            "12/01/2019 09:34:52 - INFO - root -   train_loss after step 17750: 0.3553159216046333\n",
            "12/01/2019 09:35:20 - INFO - root -   lr after step 17800: 2.9756319791244485e-05\n",
            "12/01/2019 09:35:20 - INFO - root -   train_loss after step 17800: 0.37587314724922183\n",
            "12/01/2019 09:35:48 - INFO - root -   lr after step 17850: 2.9619426820744437e-05\n",
            "12/01/2019 09:35:48 - INFO - root -   train_loss after step 17850: 0.3393843838572502\n",
            "12/01/2019 09:36:16 - INFO - root -   lr after step 17900: 2.9482541775348525e-05\n",
            "12/01/2019 09:36:16 - INFO - root -   train_loss after step 17900: 0.3851665291190147\n",
            "12/01/2019 09:36:45 - INFO - root -   lr after step 17950: 2.934566750556832e-05\n",
            "12/01/2019 09:36:45 - INFO - root -   train_loss after step 17950: 0.35444694608449934\n",
            "12/01/2019 09:37:13 - INFO - root -   lr after step 18000: 2.920880686169094e-05\n",
            "12/01/2019 09:37:13 - INFO - root -   train_loss after step 18000: 0.38460178464651107\n",
            "12/01/2019 09:37:41 - INFO - root -   lr after step 18050: 2.9071962693719826e-05\n",
            "12/01/2019 09:37:41 - INFO - root -   train_loss after step 18050: 0.3546939778327942\n",
            "12/01/2019 09:38:09 - INFO - root -   lr after step 18100: 2.893513785131526e-05\n",
            "12/01/2019 09:38:09 - INFO - root -   train_loss after step 18100: 0.37573428332805636\n",
            "12/01/2019 09:38:37 - INFO - root -   lr after step 18150: 2.8798335183735115e-05\n",
            "12/01/2019 09:38:37 - INFO - root -   train_loss after step 18150: 0.3556400328874588\n",
            "12/01/2019 09:39:05 - INFO - root -   lr after step 18200: 2.8661557539775495e-05\n",
            "12/01/2019 09:39:05 - INFO - root -   train_loss after step 18200: 0.36486679434776303\n",
            "12/01/2019 09:39:34 - INFO - root -   lr after step 18250: 2.8524807767711403e-05\n",
            "12/01/2019 09:39:34 - INFO - root -   train_loss after step 18250: 0.375548400580883\n",
            "12/01/2019 09:40:02 - INFO - root -   lr after step 18300: 2.8388088715237433e-05\n",
            "12/01/2019 09:40:02 - INFO - root -   train_loss after step 18300: 0.37980049967765805\n",
            "12/01/2019 09:40:30 - INFO - root -   lr after step 18350: 2.8251403229408462e-05\n",
            "12/01/2019 09:40:30 - INFO - root -   train_loss after step 18350: 0.368868091404438\n",
            "12/01/2019 09:40:58 - INFO - root -   lr after step 18400: 2.81147541565804e-05\n",
            "12/01/2019 09:40:58 - INFO - root -   train_loss after step 18400: 0.3419063585996628\n",
            "12/01/2019 09:41:26 - INFO - root -   lr after step 18450: 2.7978144342350862e-05\n",
            "12/01/2019 09:41:26 - INFO - root -   train_loss after step 18450: 0.3501737678050995\n",
            "12/01/2019 09:41:54 - INFO - root -   lr after step 18500: 2.7841576631499924e-05\n",
            "12/01/2019 09:41:54 - INFO - root -   train_loss after step 18500: 0.35699609279632566\n",
            "12/01/2019 09:42:22 - INFO - root -   lr after step 18550: 2.7705053867930946e-05\n",
            "12/01/2019 09:42:22 - INFO - root -   train_loss after step 18550: 0.3529825237393379\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 09:42:50 - INFO - root -   lr after step 18600: 2.7568578894611244e-05\n",
            "12/01/2019 09:42:50 - INFO - root -   train_loss after step 18600: 0.3473791179060936\n",
            "12/01/2019 09:43:18 - INFO - root -   lr after step 18650: 2.743215455351299e-05\n",
            "12/01/2019 09:43:18 - INFO - root -   train_loss after step 18650: 0.37816543638706207\n",
            "12/01/2019 09:43:47 - INFO - root -   lr after step 18700: 2.7295783685553957e-05\n",
            "12/01/2019 09:43:47 - INFO - root -   train_loss after step 18700: 0.3564168468117714\n",
            "12/01/2019 09:44:15 - INFO - root -   lr after step 18750: 2.7159469130538377e-05\n",
            "12/01/2019 09:44:15 - INFO - root -   train_loss after step 18750: 0.36908758819103243\n",
            "12/01/2019 09:44:43 - INFO - root -   lr after step 18800: 2.7023213727097854e-05\n",
            "12/01/2019 09:44:43 - INFO - root -   train_loss after step 18800: 0.357959785759449\n",
            "12/01/2019 09:45:11 - INFO - root -   lr after step 18850: 2.688702031263217e-05\n",
            "12/01/2019 09:45:11 - INFO - root -   train_loss after step 18850: 0.36015899896621706\n",
            "12/01/2019 09:45:39 - INFO - root -   lr after step 18900: 2.675089172325028e-05\n",
            "12/01/2019 09:45:39 - INFO - root -   train_loss after step 18900: 0.385445536673069\n",
            "12/01/2019 09:46:07 - INFO - root -   lr after step 18950: 2.6614830793711173e-05\n",
            "12/01/2019 09:46:07 - INFO - root -   train_loss after step 18950: 0.3661618682742119\n",
            "12/01/2019 09:46:35 - INFO - root -   lr after step 19000: 2.6478840357364905e-05\n",
            "12/01/2019 09:46:35 - INFO - root -   train_loss after step 19000: 0.36081563770771025\n",
            "12/01/2019 09:47:04 - INFO - root -   lr after step 19050: 2.634292324609358e-05\n",
            "12/01/2019 09:47:04 - INFO - root -   train_loss after step 19050: 0.34314610809087753\n",
            "12/01/2019 09:47:32 - INFO - root -   lr after step 19100: 2.6207082290252342e-05\n",
            "12/01/2019 09:47:32 - INFO - root -   train_loss after step 19100: 0.36473574981093404\n",
            "12/01/2019 09:48:00 - INFO - root -   lr after step 19150: 2.6071320318610498e-05\n",
            "12/01/2019 09:48:00 - INFO - root -   train_loss after step 19150: 0.3340654790401459\n",
            "12/01/2019 09:48:28 - INFO - root -   lr after step 19200: 2.5935640158292537e-05\n",
            "12/01/2019 09:48:28 - INFO - root -   train_loss after step 19200: 0.34975491255521773\n",
            "12/01/2019 09:48:56 - INFO - root -   lr after step 19250: 2.580004463471936e-05\n",
            "12/01/2019 09:48:56 - INFO - root -   train_loss after step 19250: 0.3651094946265221\n",
            "12/01/2019 09:49:25 - INFO - root -   lr after step 19300: 2.5664536571549318e-05\n",
            "12/01/2019 09:49:25 - INFO - root -   train_loss after step 19300: 0.3540136530995369\n",
            "12/01/2019 09:49:53 - INFO - root -   lr after step 19350: 2.55291187906195e-05\n",
            "12/01/2019 09:49:53 - INFO - root -   train_loss after step 19350: 0.3719282057881355\n",
            "12/01/2019 09:50:21 - INFO - root -   lr after step 19400: 2.5393794111886975e-05\n",
            "12/01/2019 09:50:21 - INFO - root -   train_loss after step 19400: 0.35087161749601364\n",
            "12/01/2019 09:50:49 - INFO - root -   lr after step 19450: 2.525856535336999e-05\n",
            "12/01/2019 09:50:49 - INFO - root -   train_loss after step 19450: 0.3403296811878681\n",
            "12/01/2019 09:51:18 - INFO - root -   lr after step 19500: 2.5123435331089377e-05\n",
            "12/01/2019 09:51:18 - INFO - root -   train_loss after step 19500: 0.38314036577939986\n",
            "12/01/2019 09:51:46 - INFO - root -   lr after step 19550: 2.4988406859009843e-05\n",
            "12/01/2019 09:51:46 - INFO - root -   train_loss after step 19550: 0.3657430449128151\n",
            "12/01/2019 09:52:14 - INFO - root -   lr after step 19600: 2.4853482748981432e-05\n",
            "12/01/2019 09:52:14 - INFO - root -   train_loss after step 19600: 0.37131223946809766\n",
            "12/01/2019 09:52:42 - INFO - root -   lr after step 19650: 2.4718665810680907e-05\n",
            "12/01/2019 09:52:42 - INFO - root -   train_loss after step 19650: 0.34750196695327756\n",
            "12/01/2019 09:53:11 - INFO - root -   lr after step 19700: 2.458395885155328e-05\n",
            "12/01/2019 09:53:11 - INFO - root -   train_loss after step 19700: 0.38540398478508\n",
            "12/01/2019 09:53:39 - INFO - root -   lr after step 19750: 2.444936467675338e-05\n",
            "12/01/2019 09:53:39 - INFO - root -   train_loss after step 19750: 0.37099909216165544\n",
            "12/01/2019 09:54:07 - INFO - root -   lr after step 19800: 2.4314886089087346e-05\n",
            "12/01/2019 09:54:07 - INFO - root -   train_loss after step 19800: 0.3911828911304474\n",
            "12/01/2019 09:54:35 - INFO - root -   lr after step 19850: 2.418052588895435e-05\n",
            "12/01/2019 09:54:35 - INFO - root -   train_loss after step 19850: 0.3677848756313324\n",
            "12/01/2019 09:55:04 - INFO - root -   lr after step 19900: 2.4046286874288237e-05\n",
            "12/01/2019 09:55:04 - INFO - root -   train_loss after step 19900: 0.36626597195863725\n",
            "12/01/2019 09:55:32 - INFO - root -   lr after step 19950: 2.3912171840499272e-05\n",
            "12/01/2019 09:55:32 - INFO - root -   train_loss after step 19950: 0.34879365205764773\n",
            "12/01/2019 09:56:00 - INFO - root -   lr after step 20000: 2.3778183580415933e-05\n",
            "12/01/2019 09:56:00 - INFO - root -   train_loss after step 20000: 0.3554075038433075\n",
            "12/01/2019 09:56:28 - INFO - root -   lr after step 20050: 2.364432488422672e-05\n",
            "12/01/2019 09:56:28 - INFO - root -   train_loss after step 20050: 0.3758740645647049\n",
            "12/01/2019 09:56:56 - INFO - root -   lr after step 20100: 2.3510598539422124e-05\n",
            "12/01/2019 09:56:56 - INFO - root -   train_loss after step 20100: 0.3444957613945007\n",
            "12/01/2019 09:57:25 - INFO - root -   lr after step 20150: 2.33770073307365e-05\n",
            "12/01/2019 09:57:25 - INFO - root -   train_loss after step 20150: 0.3527641862630844\n",
            "12/01/2019 09:57:53 - INFO - root -   lr after step 20200: 2.3243554040090096e-05\n",
            "12/01/2019 09:57:53 - INFO - root -   train_loss after step 20200: 0.37457051187753676\n",
            "12/01/2019 09:58:21 - INFO - root -   lr after step 20250: 2.3110241446531196e-05\n",
            "12/01/2019 09:58:21 - INFO - root -   train_loss after step 20250: 0.3378490337729454\n",
            "12/01/2019 09:58:49 - INFO - root -   lr after step 20300: 2.2977072326178118e-05\n",
            "12/01/2019 09:58:49 - INFO - root -   train_loss after step 20300: 0.3665362975001335\n",
            "12/01/2019 09:59:17 - INFO - root -   lr after step 20350: 2.284404945216155e-05\n",
            "12/01/2019 09:59:17 - INFO - root -   train_loss after step 20350: 0.3870759376883507\n",
            "12/01/2019 09:59:46 - INFO - root -   lr after step 20400: 2.271117559456665e-05\n",
            "12/01/2019 09:59:46 - INFO - root -   train_loss after step 20400: 0.36444820791482924\n",
            "12/01/2019 10:00:14 - INFO - root -   lr after step 20450: 2.257845352037552e-05\n",
            "12/01/2019 10:00:14 - INFO - root -   train_loss after step 20450: 0.3400978723168373\n",
            "12/01/2019 10:00:42 - INFO - root -   lr after step 20500: 2.244588599340944e-05\n",
            "12/01/2019 10:00:42 - INFO - root -   train_loss after step 20500: 0.37305102676153185\n",
            "12/01/2019 10:01:10 - INFO - root -   lr after step 20550: 2.23134757742714e-05\n",
            "12/01/2019 10:01:10 - INFO - root -   train_loss after step 20550: 0.366486160159111\n",
            "12/01/2019 10:01:38 - INFO - root -   lr after step 20600: 2.2181225620288602e-05\n",
            "12/01/2019 10:01:38 - INFO - root -   train_loss after step 20600: 0.364328079521656\n",
            "12/01/2019 10:02:06 - INFO - root -   lr after step 20650: 2.2049138285455025e-05\n",
            "12/01/2019 10:02:06 - INFO - root -   train_loss after step 20650: 0.34766572803258894\n",
            "12/01/2019 10:02:35 - INFO - root -   lr after step 20700: 2.1917216520374083e-05\n",
            "12/01/2019 10:02:35 - INFO - root -   train_loss after step 20700: 0.3632347556948662\n",
            "12/01/2019 10:03:03 - INFO - root -   lr after step 20750: 2.1785463072201338e-05\n",
            "12/01/2019 10:03:03 - INFO - root -   train_loss after step 20750: 0.3318660381436348\n",
            "12/01/2019 10:03:31 - INFO - root -   lr after step 20800: 2.165388068458729e-05\n",
            "12/01/2019 10:03:31 - INFO - root -   train_loss after step 20800: 0.37003846555948255\n",
            "12/01/2019 10:03:59 - INFO - root -   lr after step 20850: 2.1522472097620292e-05\n",
            "12/01/2019 10:03:59 - INFO - root -   train_loss after step 20850: 0.36037358313798906\n",
            "12/01/2019 10:04:27 - INFO - root -   lr after step 20900: 2.139124004776939e-05\n",
            "12/01/2019 10:04:27 - INFO - root -   train_loss after step 20900: 0.3602185297012329\n",
            "12/01/2019 10:04:55 - INFO - root -   lr after step 20950: 2.126018726782747e-05\n",
            "12/01/2019 10:04:55 - INFO - root -   train_loss after step 20950: 0.38245810717344286\n",
            "12/01/2019 10:05:23 - INFO - root -   lr after step 21000: 2.112931648685422e-05\n",
            "12/01/2019 10:05:23 - INFO - root -   train_loss after step 21000: 0.325671666264534\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 10:05:51 - INFO - root -   lr after step 21050: 2.0998630430119375e-05\n",
            "12/01/2019 10:05:51 - INFO - root -   train_loss after step 21050: 0.35584390074014666\n",
            "12/01/2019 10:06:20 - INFO - root -   lr after step 21100: 2.0868131819045965e-05\n",
            "12/01/2019 10:06:20 - INFO - root -   train_loss after step 21100: 0.34791518598794935\n",
            "12/01/2019 10:06:48 - INFO - root -   lr after step 21150: 2.0737823371153598e-05\n",
            "12/01/2019 10:06:48 - INFO - root -   train_loss after step 21150: 0.33095623075962066\n",
            "12/01/2019 10:07:16 - INFO - root -   lr after step 21200: 2.0607707800001953e-05\n",
            "12/01/2019 10:07:16 - INFO - root -   train_loss after step 21200: 0.3551520684361458\n",
            "12/01/2019 10:07:44 - INFO - root -   lr after step 21250: 2.0477787815134147e-05\n",
            "12/01/2019 10:07:44 - INFO - root -   train_loss after step 21250: 0.33826877385377885\n",
            "12/01/2019 10:08:13 - INFO - root -   lr after step 21300: 2.034806612202044e-05\n",
            "12/01/2019 10:08:13 - INFO - root -   train_loss after step 21300: 0.3915923514962196\n",
            "12/01/2019 10:08:41 - INFO - root -   lr after step 21350: 2.0218545422001822e-05\n",
            "12/01/2019 10:08:41 - INFO - root -   train_loss after step 21350: 0.36012853994965555\n",
            "12/01/2019 10:09:09 - INFO - root -   lr after step 21400: 2.008922841223376e-05\n",
            "12/01/2019 10:09:09 - INFO - root -   train_loss after step 21400: 0.3599324989318848\n",
            "12/01/2019 10:09:37 - INFO - root -   lr after step 21450: 1.9960117785630077e-05\n",
            "12/01/2019 10:09:37 - INFO - root -   train_loss after step 21450: 0.35716102898120883\n",
            "12/01/2019 10:10:05 - INFO - root -   lr after step 21500: 1.9831216230806833e-05\n",
            "12/01/2019 10:10:05 - INFO - root -   train_loss after step 21500: 0.3553845700621605\n",
            "12/01/2019 10:10:34 - INFO - root -   lr after step 21550: 1.9702526432026357e-05\n",
            "12/01/2019 10:10:34 - INFO - root -   train_loss after step 21550: 0.3801879128813744\n",
            "12/01/2019 10:11:02 - INFO - root -   lr after step 21600: 1.9574051069141333e-05\n",
            "12/01/2019 10:11:02 - INFO - root -   train_loss after step 21600: 0.36699857145547865\n",
            "12/01/2019 10:11:30 - INFO - root -   lr after step 21650: 1.9445792817538993e-05\n",
            "12/01/2019 10:11:30 - INFO - root -   train_loss after step 21650: 0.3527154916524887\n",
            "12/01/2019 10:11:58 - INFO - root -   lr after step 21700: 1.9317754348085466e-05\n",
            "12/01/2019 10:11:58 - INFO - root -   train_loss after step 21700: 0.3533212476968765\n",
            "12/01/2019 10:12:26 - INFO - root -   lr after step 21750: 1.9189938327070048e-05\n",
            "12/01/2019 10:12:26 - INFO - root -   train_loss after step 21750: 0.35225128412246703\n",
            "12/01/2019 10:12:54 - INFO - root -   lr after step 21800: 1.9062347416149795e-05\n",
            "12/01/2019 10:12:54 - INFO - root -   train_loss after step 21800: 0.34718341767787936\n",
            "12/01/2019 10:13:23 - INFO - root -   lr after step 21850: 1.893498427229399e-05\n",
            "12/01/2019 10:13:23 - INFO - root -   train_loss after step 21850: 0.36244438171386717\n",
            "12/01/2019 10:13:51 - INFO - root -   lr after step 21900: 1.8807851547728925e-05\n",
            "12/01/2019 10:13:51 - INFO - root -   train_loss after step 21900: 0.35522819876670836\n",
            "12/01/2019 10:14:19 - INFO - root -   lr after step 21950: 1.8680951889882554e-05\n",
            "12/01/2019 10:14:19 - INFO - root -   train_loss after step 21950: 0.33584054261446\n",
            "12/01/2019 10:14:47 - INFO - root -   lr after step 22000: 1.8554287941329443e-05\n",
            "12/01/2019 10:14:47 - INFO - root -   train_loss after step 22000: 0.33271159082651136\n",
            "12/01/2019 10:15:15 - INFO - root -   lr after step 22050: 1.8427862339735736e-05\n",
            "12/01/2019 10:15:15 - INFO - root -   train_loss after step 22050: 0.35322154939174655\n",
            "12/01/2019 10:15:43 - INFO - root -   lr after step 22100: 1.8301677717804178e-05\n",
            "12/01/2019 10:15:43 - INFO - root -   train_loss after step 22100: 0.3893507844209671\n",
            "12/01/2019 10:16:12 - INFO - root -   lr after step 22150: 1.817573670321935e-05\n",
            "12/01/2019 10:16:12 - INFO - root -   train_loss after step 22150: 0.34989699244499206\n",
            "12/01/2019 10:16:40 - INFO - root -   lr after step 22200: 1.8050041918592907e-05\n",
            "12/01/2019 10:16:40 - INFO - root -   train_loss after step 22200: 0.3584461063146591\n",
            "12/01/2019 10:17:08 - INFO - root -   lr after step 22250: 1.7924595981408982e-05\n",
            "12/01/2019 10:17:08 - INFO - root -   train_loss after step 22250: 0.37116580963134765\n",
            "12/01/2019 10:17:36 - INFO - root -   lr after step 22300: 1.7799401503969683e-05\n",
            "12/01/2019 10:17:36 - INFO - root -   train_loss after step 22300: 0.35640106528997423\n",
            "12/01/2019 10:18:04 - INFO - root -   lr after step 22350: 1.7674461093340667e-05\n",
            "12/01/2019 10:18:04 - INFO - root -   train_loss after step 22350: 0.3684903731942177\n",
            "12/01/2019 10:18:32 - INFO - root -   lr after step 22400: 1.754977735129692e-05\n",
            "12/01/2019 10:18:32 - INFO - root -   train_loss after step 22400: 0.3629907765984535\n",
            "12/01/2019 10:19:00 - INFO - root -   lr after step 22450: 1.7425352874268478e-05\n",
            "12/01/2019 10:19:00 - INFO - root -   train_loss after step 22450: 0.38967484921216966\n",
            "12/01/2019 10:19:29 - INFO - root -   lr after step 22500: 1.7301190253286425e-05\n",
            "12/01/2019 10:19:29 - INFO - root -   train_loss after step 22500: 0.3634994548559189\n",
            "12/01/2019 10:19:57 - INFO - root -   lr after step 22550: 1.7177292073928945e-05\n",
            "12/01/2019 10:19:57 - INFO - root -   train_loss after step 22550: 0.3784562194347382\n",
            "12/01/2019 10:20:25 - INFO - root -   lr after step 22600: 1.7053660916267414e-05\n",
            "12/01/2019 10:20:25 - INFO - root -   train_loss after step 22600: 0.3706738340854645\n",
            "12/01/2019 10:20:53 - INFO - root -   lr after step 22650: 1.6930299354812767e-05\n",
            "12/01/2019 10:20:53 - INFO - root -   train_loss after step 22650: 0.35328258454799655\n",
            "12/01/2019 10:21:21 - INFO - root -   lr after step 22700: 1.680720995846177e-05\n",
            "12/01/2019 10:21:21 - INFO - root -   train_loss after step 22700: 0.3296639397740364\n",
            "12/01/2019 10:21:50 - INFO - root -   lr after step 22750: 1.6684395290443653e-05\n",
            "12/01/2019 10:21:50 - INFO - root -   train_loss after step 22750: 0.3703594395518303\n",
            "12/01/2019 10:22:18 - INFO - root -   lr after step 22800: 1.6561857908266623e-05\n",
            "12/01/2019 10:22:18 - INFO - root -   train_loss after step 22800: 0.3460246828198433\n",
            "12/01/2019 10:22:46 - INFO - root -   lr after step 22850: 1.6439600363664652e-05\n",
            "12/01/2019 10:22:46 - INFO - root -   train_loss after step 22850: 0.3555408430099487\n",
            "12/01/2019 10:23:14 - INFO - root -   lr after step 22900: 1.6317625202544385e-05\n",
            "12/01/2019 10:23:14 - INFO - root -   train_loss after step 22900: 0.37495826065540316\n",
            "12/01/2019 10:23:43 - INFO - root -   lr after step 22950: 1.619593496493201e-05\n",
            "12/01/2019 10:23:43 - INFO - root -   train_loss after step 22950: 0.3634875389933586\n",
            "12/01/2019 10:24:11 - INFO - root -   lr after step 23000: 1.60745321849205e-05\n",
            "12/01/2019 10:24:11 - INFO - root -   train_loss after step 23000: 0.37244623452425\n",
            "12/01/2019 10:24:39 - INFO - root -   lr after step 23050: 1.5953419390616722e-05\n",
            "12/01/2019 10:24:39 - INFO - root -   train_loss after step 23050: 0.34736605614423754\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 10:25:07 - INFO - root -   lr after step 23100: 1.5832599104088866e-05\n",
            "12/01/2019 10:25:07 - INFO - root -   train_loss after step 23100: 0.3616856959462166\n",
            "12/01/2019 10:25:35 - INFO - root -   lr after step 23150: 1.5712073841313904e-05\n",
            "12/01/2019 10:25:35 - INFO - root -   train_loss after step 23150: 0.36392896354198456\n",
            "12/01/2019 10:26:03 - INFO - root -   lr after step 23200: 1.5591846112125185e-05\n",
            "12/01/2019 10:26:03 - INFO - root -   train_loss after step 23200: 0.36065647736191747\n",
            "12/01/2019 10:26:31 - INFO - root -   lr after step 23250: 1.5471918420160206e-05\n",
            "12/01/2019 10:26:31 - INFO - root -   train_loss after step 23250: 0.3539828556776047\n",
            "12/01/2019 10:26:59 - INFO - root -   lr after step 23300: 1.5352293262808417e-05\n",
            "12/01/2019 10:26:59 - INFO - root -   train_loss after step 23300: 0.3457741394639015\n",
            "12/01/2019 10:27:27 - INFO - root -   lr after step 23350: 1.5232973131159275e-05\n",
            "12/01/2019 10:27:27 - INFO - root -   train_loss after step 23350: 0.36674713909626006\n",
            "12/01/2019 10:27:56 - INFO - root -   lr after step 23400: 1.5113960509950311e-05\n",
            "12/01/2019 10:27:56 - INFO - root -   train_loss after step 23400: 0.3544334203004837\n",
            "12/01/2019 10:28:24 - INFO - root -   lr after step 23450: 1.4995257877515472e-05\n",
            "12/01/2019 10:28:24 - INFO - root -   train_loss after step 23450: 0.3664197912812233\n",
            "12/01/2019 10:28:52 - INFO - root -   lr after step 23500: 1.4876867705733416e-05\n",
            "12/01/2019 10:28:52 - INFO - root -   train_loss after step 23500: 0.35577467292547227\n",
            "12/01/2019 10:29:20 - INFO - root -   lr after step 23550: 1.4758792459976077e-05\n",
            "12/01/2019 10:29:20 - INFO - root -   train_loss after step 23550: 0.3511097565293312\n",
            "12/01/2019 10:29:49 - INFO - root -   lr after step 23600: 1.464103459905738e-05\n",
            "12/01/2019 10:29:49 - INFO - root -   train_loss after step 23600: 0.3690719467401504\n",
            "12/01/2019 10:30:17 - INFO - root -   lr after step 23650: 1.4523596575181931e-05\n",
            "12/01/2019 10:30:17 - INFO - root -   train_loss after step 23650: 0.37408404737710954\n",
            "12/01/2019 10:30:45 - INFO - root -   lr after step 23700: 1.4406480833894027e-05\n",
            "12/01/2019 10:30:45 - INFO - root -   train_loss after step 23700: 0.3998331460356712\n",
            "12/01/2019 10:31:13 - INFO - root -   lr after step 23750: 1.4289689814026738e-05\n",
            "12/01/2019 10:31:13 - INFO - root -   train_loss after step 23750: 0.3382741430401802\n",
            "12/01/2019 10:31:41 - INFO - root -   lr after step 23800: 1.4173225947651051e-05\n",
            "12/01/2019 10:31:41 - INFO - root -   train_loss after step 23800: 0.3780868574976921\n",
            "12/01/2019 10:32:09 - INFO - root -   lr after step 23850: 1.4057091660025311e-05\n",
            "12/01/2019 10:32:09 - INFO - root -   train_loss after step 23850: 0.3735357704758644\n",
            "12/01/2019 10:32:38 - INFO - root -   lr after step 23900: 1.394128936954464e-05\n",
            "12/01/2019 10:32:38 - INFO - root -   train_loss after step 23900: 0.3481785762310028\n",
            "12/01/2019 10:33:06 - INFO - root -   lr after step 23950: 1.3825821487690608e-05\n",
            "12/01/2019 10:33:06 - INFO - root -   train_loss after step 23950: 0.39191240549087525\n",
            "12/01/2019 10:33:34 - INFO - root -   lr after step 24000: 1.3710690418981053e-05\n",
            "12/01/2019 10:33:34 - INFO - root -   train_loss after step 24000: 0.3602690181136131\n",
            "12/01/2019 10:34:02 - INFO - root -   lr after step 24050: 1.3595898560919926e-05\n",
            "12/01/2019 10:34:02 - INFO - root -   train_loss after step 24050: 0.3690680775046349\n",
            "12/01/2019 10:34:30 - INFO - root -   lr after step 24100: 1.3481448303947459e-05\n",
            "12/01/2019 10:34:30 - INFO - root -   train_loss after step 24100: 0.3441362205147743\n",
            "12/01/2019 10:34:58 - INFO - root -   lr after step 24150: 1.33673420313903e-05\n",
            "12/01/2019 10:34:58 - INFO - root -   train_loss after step 24150: 0.37116308242082596\n",
            "12/01/2019 10:35:26 - INFO - root -   lr after step 24200: 1.3253582119411956e-05\n",
            "12/01/2019 10:35:26 - INFO - root -   train_loss after step 24200: 0.371936067044735\n",
            "12/01/2019 10:35:55 - INFO - root -   lr after step 24250: 1.314017093696325e-05\n",
            "12/01/2019 10:35:55 - INFO - root -   train_loss after step 24250: 0.3597907653450966\n",
            "12/01/2019 10:36:23 - INFO - root -   lr after step 24300: 1.302711084573302e-05\n",
            "12/01/2019 10:36:23 - INFO - root -   train_loss after step 24300: 0.35762809693813324\n",
            "12/01/2019 10:36:51 - INFO - root -   lr after step 24350: 1.2914404200098961e-05\n",
            "12/01/2019 10:36:51 - INFO - root -   train_loss after step 24350: 0.3380731183290482\n",
            "12/01/2019 10:37:19 - INFO - root -   lr after step 24400: 1.2802053347078534e-05\n",
            "12/01/2019 10:37:19 - INFO - root -   train_loss after step 24400: 0.3824146792292595\n",
            "12/01/2019 10:37:47 - INFO - root -   lr after step 24450: 1.2690060626280174e-05\n",
            "12/01/2019 10:37:47 - INFO - root -   train_loss after step 24450: 0.35364477038383485\n",
            "12/01/2019 10:38:15 - INFO - root -   lr after step 24500: 1.2578428369854488e-05\n",
            "12/01/2019 10:38:15 - INFO - root -   train_loss after step 24500: 0.37538054525852205\n",
            "12/01/2019 10:38:44 - INFO - root -   lr after step 24550: 1.246715890244573e-05\n",
            "12/01/2019 10:38:44 - INFO - root -   train_loss after step 24550: 0.3533967140316963\n",
            "12/01/2019 10:39:12 - INFO - root -   lr after step 24600: 1.2356254541143425e-05\n",
            "12/01/2019 10:39:12 - INFO - root -   train_loss after step 24600: 0.35860232532024383\n",
            "12/01/2019 10:39:40 - INFO - root -   lr after step 24650: 1.2245717595434028e-05\n",
            "12/01/2019 10:39:40 - INFO - root -   train_loss after step 24650: 0.3607729497551918\n",
            "12/01/2019 10:40:09 - INFO - root -   lr after step 24700: 1.2135550367152944e-05\n",
            "12/01/2019 10:40:09 - INFO - root -   train_loss after step 24700: 0.37916882365942\n",
            "12/01/2019 10:40:37 - INFO - root -   lr after step 24750: 1.2025755150436487e-05\n",
            "12/01/2019 10:40:37 - INFO - root -   train_loss after step 24750: 0.3720933470129967\n",
            "12/01/2019 10:41:05 - INFO - root -   lr after step 24800: 1.1916334231674171e-05\n",
            "12/01/2019 10:41:05 - INFO - root -   train_loss after step 24800: 0.3583399894833565\n",
            "12/01/2019 10:41:34 - INFO - root -   lr after step 24850: 1.1807289889461102e-05\n",
            "12/01/2019 10:41:34 - INFO - root -   train_loss after step 24850: 0.33883374541997907\n",
            "12/01/2019 10:42:02 - INFO - root -   lr after step 24900: 1.1698624394550475e-05\n",
            "12/01/2019 10:42:02 - INFO - root -   train_loss after step 24900: 0.388452767431736\n",
            "12/01/2019 10:42:30 - INFO - root -   lr after step 24950: 1.1590340009806333e-05\n",
            "12/01/2019 10:42:30 - INFO - root -   train_loss after step 24950: 0.3299046319723129\n",
            "12/01/2019 10:42:59 - INFO - root -   lr after step 25000: 1.1482438990156454e-05\n",
            "12/01/2019 10:42:59 - INFO - root -   train_loss after step 25000: 0.3627967591583729\n",
            "12/01/2019 10:43:27 - INFO - root -   lr after step 25050: 1.1374923582545346e-05\n",
            "12/01/2019 10:43:27 - INFO - root -   train_loss after step 25050: 0.3648774927854538\n",
            "12/01/2019 10:43:56 - INFO - root -   lr after step 25100: 1.1267796025887492e-05\n",
            "12/01/2019 10:43:56 - INFO - root -   train_loss after step 25100: 0.3670165804028511\n",
            "12/01/2019 10:44:24 - INFO - root -   lr after step 25150: 1.1161058551020701e-05\n",
            "12/01/2019 10:44:24 - INFO - root -   train_loss after step 25150: 0.35493857994675637\n",
            "12/01/2019 10:44:52 - INFO - root -   lr after step 25200: 1.1054713380659713e-05\n",
            "12/01/2019 10:44:52 - INFO - root -   train_loss after step 25200: 0.3670493081212044\n",
            "12/01/2019 10:45:21 - INFO - root -   lr after step 25250: 1.0948762729349819e-05\n",
            "12/01/2019 10:45:21 - INFO - root -   train_loss after step 25250: 0.3583649110794067\n",
            "12/01/2019 10:45:49 - INFO - root -   lr after step 25300: 1.0843208803420839e-05\n",
            "12/01/2019 10:45:49 - INFO - root -   train_loss after step 25300: 0.3921975141763687\n",
            "12/01/2019 10:46:18 - INFO - root -   lr after step 25350: 1.0738053800941098e-05\n",
            "12/01/2019 10:46:18 - INFO - root -   train_loss after step 25350: 0.35527833431959155\n",
            "12/01/2019 10:46:46 - INFO - root -   lr after step 25400: 1.0633299911671696e-05\n",
            "12/01/2019 10:46:46 - INFO - root -   train_loss after step 25400: 0.3692739939689636\n",
            "12/01/2019 10:47:14 - INFO - root -   lr after step 25450: 1.052894931702093e-05\n",
            "12/01/2019 10:47:14 - INFO - root -   train_loss after step 25450: 0.3950856223702431\n",
            "12/01/2019 10:47:43 - INFO - root -   lr after step 25500: 1.0425004189998792e-05\n",
            "12/01/2019 10:47:43 - INFO - root -   train_loss after step 25500: 0.345217641890049\n",
            "12/01/2019 10:48:12 - INFO - root -   lr after step 25550: 1.032146669517181e-05\n",
            "12/01/2019 10:48:12 - INFO - root -   train_loss after step 25550: 0.3419433957338333\n",
            "12/01/2019 10:48:40 - INFO - root -   lr after step 25600: 1.021833898861789e-05\n",
            "12/01/2019 10:48:40 - INFO - root -   train_loss after step 25600: 0.3754932987689972\n",
            "12/01/2019 10:49:08 - INFO - root -   lr after step 25650: 1.0115623217881487e-05\n",
            "12/01/2019 10:49:08 - INFO - root -   train_loss after step 25650: 0.35640464574098585\n",
            "12/01/2019 10:49:37 - INFO - root -   lr after step 25700: 1.0013321521928827e-05\n",
            "12/01/2019 10:49:37 - INFO - root -   train_loss after step 25700: 0.337475049495697\n",
            "12/01/2019 10:50:05 - INFO - root -   lr after step 25750: 9.911436031103392e-06\n",
            "12/01/2019 10:50:05 - INFO - root -   train_loss after step 25750: 0.3619199004769325\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 10:50:34 - INFO - root -   lr after step 25800: 9.809968867081582e-06\n",
            "12/01/2019 10:50:34 - INFO - root -   train_loss after step 25800: 0.33921130269765853\n",
            "12/01/2019 10:51:02 - INFO - root -   lr after step 25850: 9.70892214282847e-06\n",
            "12/01/2019 10:51:02 - INFO - root -   train_loss after step 25850: 0.36147151023149493\n",
            "12/01/2019 10:51:31 - INFO - root -   lr after step 25900: 9.608297962553871e-06\n",
            "12/01/2019 10:51:31 - INFO - root -   train_loss after step 25900: 0.3530147495865822\n",
            "12/01/2019 10:51:59 - INFO - root -   lr after step 25950: 9.508098421668474e-06\n",
            "12/01/2019 10:51:59 - INFO - root -   train_loss after step 25950: 0.3553591457009315\n",
            "12/01/2019 10:52:28 - INFO - root -   lr after step 26000: 9.408325606740206e-06\n",
            "12/01/2019 10:52:28 - INFO - root -   train_loss after step 26000: 0.35616920977830885\n",
            "12/01/2019 10:52:56 - INFO - root -   lr after step 26050: 9.308981595450846e-06\n",
            "12/01/2019 10:52:56 - INFO - root -   train_loss after step 26050: 0.33948989033699034\n",
            "12/01/2019 10:53:25 - INFO - root -   lr after step 26100: 9.210068456552659e-06\n",
            "12/01/2019 10:53:25 - INFO - root -   train_loss after step 26100: 0.3356493890285492\n",
            "12/01/2019 10:53:53 - INFO - root -   lr after step 26150: 9.111588249825413e-06\n",
            "12/01/2019 10:53:53 - INFO - root -   train_loss after step 26150: 0.35164827942848204\n",
            "12/01/2019 10:54:22 - INFO - root -   lr after step 26200: 9.013543026033412e-06\n",
            "12/01/2019 10:54:22 - INFO - root -   train_loss after step 26200: 0.36384265214204786\n",
            "12/01/2019 10:54:50 - INFO - root -   lr after step 26250: 8.915934826882821e-06\n",
            "12/01/2019 10:54:50 - INFO - root -   train_loss after step 26250: 0.35283153876662254\n",
            "12/01/2019 10:55:19 - INFO - root -   lr after step 26300: 8.818765684979175e-06\n",
            "12/01/2019 10:55:19 - INFO - root -   train_loss after step 26300: 0.35020660012960436\n",
            "12/01/2019 10:55:48 - INFO - root -   lr after step 26350: 8.72203762378499e-06\n",
            "12/01/2019 10:55:48 - INFO - root -   train_loss after step 26350: 0.3840648639202118\n",
            "12/01/2019 10:56:16 - INFO - root -   lr after step 26400: 8.6257526575777e-06\n",
            "12/01/2019 10:56:16 - INFO - root -   train_loss after step 26400: 0.3362826502323151\n",
            "12/01/2019 10:56:45 - INFO - root -   lr after step 26450: 8.529912791407635e-06\n",
            "12/01/2019 10:56:45 - INFO - root -   train_loss after step 26450: 0.37191375583410263\n",
            "12/01/2019 10:57:13 - INFO - root -   lr after step 26500: 8.43452002105633e-06\n",
            "12/01/2019 10:57:13 - INFO - root -   train_loss after step 26500: 0.36024455636739733\n",
            "12/01/2019 10:57:42 - INFO - root -   lr after step 26550: 8.339576332994916e-06\n",
            "12/01/2019 10:57:42 - INFO - root -   train_loss after step 26550: 0.34240720137953756\n",
            "12/01/2019 10:58:10 - INFO - root -   lr after step 26600: 8.245083704342819e-06\n",
            "12/01/2019 10:58:10 - INFO - root -   train_loss after step 26600: 0.36763301193714143\n",
            "12/01/2019 10:58:39 - INFO - root -   lr after step 26650: 8.151044102826506e-06\n",
            "12/01/2019 10:58:39 - INFO - root -   train_loss after step 26650: 0.3513336941599846\n",
            "12/01/2019 10:59:07 - INFO - root -   lr after step 26700: 8.057459486738557e-06\n",
            "12/01/2019 10:59:07 - INFO - root -   train_loss after step 26700: 0.3493873491883278\n",
            "12/01/2019 10:59:36 - INFO - root -   lr after step 26750: 7.964331804896904e-06\n",
            "12/01/2019 10:59:36 - INFO - root -   train_loss after step 26750: 0.35405676171183587\n",
            "12/01/2019 11:00:04 - INFO - root -   lr after step 26800: 7.871662996604196e-06\n",
            "12/01/2019 11:00:04 - INFO - root -   train_loss after step 26800: 0.3803785192966461\n",
            "12/01/2019 11:00:33 - INFO - root -   lr after step 26850: 7.779454991607436e-06\n",
            "12/01/2019 11:00:33 - INFO - root -   train_loss after step 26850: 0.341332283616066\n",
            "12/01/2019 11:01:01 - INFO - root -   lr after step 26900: 7.687709710057839e-06\n",
            "12/01/2019 11:01:01 - INFO - root -   train_loss after step 26900: 0.3432438379526138\n",
            "12/01/2019 11:01:29 - INFO - root -   lr after step 26950: 7.5964290624707525e-06\n",
            "12/01/2019 11:01:29 - INFO - root -   train_loss after step 26950: 0.3221085432171822\n",
            "12/01/2019 11:01:58 - INFO - root -   lr after step 27000: 7.505614949685986e-06\n",
            "12/01/2019 11:01:58 - INFO - root -   train_loss after step 27000: 0.3534114220738411\n",
            "12/01/2019 11:02:26 - INFO - root -   lr after step 27050: 7.41526926282812e-06\n",
            "12/01/2019 11:02:26 - INFO - root -   train_loss after step 27050: 0.3448163118958473\n",
            "12/01/2019 11:02:55 - INFO - root -   lr after step 27100: 7.325393883267212e-06\n",
            "12/01/2019 11:02:55 - INFO - root -   train_loss after step 27100: 0.3811145362257957\n",
            "12/01/2019 11:03:23 - INFO - root -   lr after step 27150: 7.235990682579551e-06\n",
            "12/01/2019 11:03:23 - INFO - root -   train_loss after step 27150: 0.3610992529988289\n",
            "12/01/2019 11:03:51 - INFO - root -   lr after step 27200: 7.1470615225087234e-06\n",
            "12/01/2019 11:03:51 - INFO - root -   train_loss after step 27200: 0.3464567357301712\n",
            "12/01/2019 11:04:19 - INFO - root -   lr after step 27250: 7.058608254926848e-06\n",
            "12/01/2019 11:04:19 - INFO - root -   train_loss after step 27250: 0.3914464193582535\n",
            "12/01/2019 11:04:48 - INFO - root -   lr after step 27300: 6.970632721795976e-06\n",
            "12/01/2019 11:04:48 - INFO - root -   train_loss after step 27300: 0.34025663942098616\n",
            "12/01/2019 11:05:16 - INFO - root -   lr after step 27350: 6.8831367551297754e-06\n",
            "12/01/2019 11:05:16 - INFO - root -   train_loss after step 27350: 0.38332575887441633\n",
            "12/01/2019 11:05:44 - INFO - root -   lr after step 27400: 6.796122176955347e-06\n",
            "12/01/2019 11:05:44 - INFO - root -   train_loss after step 27400: 0.3651622787117958\n",
            "12/01/2019 11:06:12 - INFO - root -   lr after step 27450: 6.709590799275289e-06\n",
            "12/01/2019 11:06:12 - INFO - root -   train_loss after step 27450: 0.3544009631872177\n",
            "12/01/2019 11:06:40 - INFO - root -   lr after step 27500: 6.623544424030008e-06\n",
            "12/01/2019 11:06:40 - INFO - root -   train_loss after step 27500: 0.3475236314535141\n",
            "12/01/2019 11:07:09 - INFO - root -   lr after step 27550: 6.537984843060113e-06\n",
            "12/01/2019 11:07:09 - INFO - root -   train_loss after step 27550: 0.3630309870839119\n",
            "12/01/2019 11:07:37 - INFO - root -   lr after step 27600: 6.452913838069182e-06\n",
            "12/01/2019 11:07:37 - INFO - root -   train_loss after step 27600: 0.37003550976514815\n",
            "12/01/2019 11:08:05 - INFO - root -   lr after step 27650: 6.368333180586609e-06\n",
            "12/01/2019 11:08:05 - INFO - root -   train_loss after step 27650: 0.3423065811395645\n",
            "12/01/2019 11:08:33 - INFO - root -   lr after step 27700: 6.28424463193072e-06\n",
            "12/01/2019 11:08:33 - INFO - root -   train_loss after step 27700: 0.34214383602142334\n",
            "12/01/2019 11:09:01 - INFO - root -   lr after step 27750: 6.200649943172132e-06\n",
            "12/01/2019 11:09:01 - INFO - root -   train_loss after step 27750: 0.36984493136405944\n",
            "12/01/2019 11:09:30 - INFO - root -   lr after step 27800: 6.117550855097229e-06\n",
            "12/01/2019 11:09:30 - INFO - root -   train_loss after step 27800: 0.35305573493242265\n",
            "12/01/2019 11:09:58 - INFO - root -   lr after step 27850: 6.034949098171976e-06\n",
            "12/01/2019 11:09:58 - INFO - root -   train_loss after step 27850: 0.3655133613944054\n",
            "12/01/2019 11:10:26 - INFO - root -   lr after step 27900: 5.9528463925058165e-06\n",
            "12/01/2019 11:10:26 - INFO - root -   train_loss after step 27900: 0.34855475842952727\n",
            "12/01/2019 11:10:54 - INFO - root -   lr after step 27950: 5.871244447815922e-06\n",
            "12/01/2019 11:10:54 - INFO - root -   train_loss after step 27950: 0.37337985903024673\n",
            "12/01/2019 11:11:22 - INFO - root -   lr after step 28000: 5.790144963391533e-06\n",
            "12/01/2019 11:11:22 - INFO - root -   train_loss after step 28000: 0.3625531962513924\n",
            "12/01/2019 11:11:51 - INFO - root -   lr after step 28050: 5.70954962805859e-06\n",
            "12/01/2019 11:11:51 - INFO - root -   train_loss after step 28050: 0.3489805978536606\n",
            "12/01/2019 11:12:19 - INFO - root -   lr after step 28100: 5.629460120144575e-06\n",
            "12/01/2019 11:12:19 - INFO - root -   train_loss after step 28100: 0.349956533908844\n",
            "12/01/2019 11:12:47 - INFO - root -   lr after step 28150: 5.54987810744357e-06\n",
            "12/01/2019 11:12:47 - INFO - root -   train_loss after step 28150: 0.3769586178660393\n",
            "12/01/2019 11:13:15 - INFO - root -   lr after step 28200: 5.470805247181495e-06\n",
            "12/01/2019 11:13:15 - INFO - root -   train_loss after step 28200: 0.3555811566114426\n",
            "12/01/2019 11:13:44 - INFO - root -   lr after step 28250: 5.392243185981618e-06\n",
            "12/01/2019 11:13:44 - INFO - root -   train_loss after step 28250: 0.369667289853096\n",
            "12/01/2019 11:14:12 - INFO - root -   lr after step 28300: 5.3141935598302514e-06\n",
            "12/01/2019 11:14:12 - INFO - root -   train_loss after step 28300: 0.36270542979240417\n",
            "12/01/2019 11:14:40 - INFO - root -   lr after step 28350: 5.236657994042732e-06\n",
            "12/01/2019 11:14:40 - INFO - root -   train_loss after step 28350: 0.3624972689151764\n",
            "12/01/2019 11:15:09 - INFO - root -   lr after step 28400: 5.159638103229494e-06\n",
            "12/01/2019 11:15:09 - INFO - root -   train_loss after step 28400: 0.38202385991811755\n",
            "12/01/2019 11:15:37 - INFO - root -   lr after step 28450: 5.083135491262529e-06\n",
            "12/01/2019 11:15:37 - INFO - root -   train_loss after step 28450: 0.347146765589714\n",
            "12/01/2019 11:16:05 - INFO - root -   lr after step 28500: 5.007151751241914e-06\n",
            "12/01/2019 11:16:05 - INFO - root -   train_loss after step 28500: 0.36168393433094026\n",
            "12/01/2019 11:16:34 - INFO - root -   lr after step 28550: 4.931688465462699e-06\n",
            "12/01/2019 11:16:34 - INFO - root -   train_loss after step 28550: 0.3623484675586224\n",
            "12/01/2019 11:17:02 - INFO - root -   lr after step 28600: 4.856747205381913e-06\n",
            "12/01/2019 11:17:02 - INFO - root -   train_loss after step 28600: 0.3812952128052711\n",
            "12/01/2019 11:17:30 - INFO - root -   lr after step 28650: 4.782329531585852e-06\n",
            "12/01/2019 11:17:30 - INFO - root -   train_loss after step 28650: 0.3544897910952568\n",
            "12/01/2019 11:17:58 - INFO - root -   lr after step 28700: 4.708436993757604e-06\n",
            "12/01/2019 11:17:58 - INFO - root -   train_loss after step 28700: 0.32621152698993683\n",
            "12/01/2019 11:18:27 - INFO - root -   lr after step 28750: 4.635071130644737e-06\n",
            "12/01/2019 11:18:27 - INFO - root -   train_loss after step 28750: 0.35258275538682937\n",
            "12/01/2019 11:18:55 - INFO - root -   lr after step 28800: 4.562233470027301e-06\n",
            "12/01/2019 11:18:55 - INFO - root -   train_loss after step 28800: 0.34360274523496626\n",
            "12/01/2019 11:19:23 - INFO - root -   lr after step 28850: 4.489925528685973e-06\n",
            "12/01/2019 11:19:23 - INFO - root -   train_loss after step 28850: 0.36975021541118624\n",
            "12/01/2019 11:19:51 - INFO - root -   lr after step 28900: 4.418148812370479e-06\n",
            "12/01/2019 11:19:51 - INFO - root -   train_loss after step 28900: 0.3710548448562622\n",
            "12/01/2019 11:20:19 - INFO - root -   lr after step 28950: 4.346904815768284e-06\n",
            "12/01/2019 11:20:19 - INFO - root -   train_loss after step 28950: 0.3686298957467079\n",
            "12/01/2019 11:20:48 - INFO - root -   lr after step 29000: 4.2761950224733895e-06\n",
            "12/01/2019 11:20:48 - INFO - root -   train_loss after step 29000: 0.3768593180179596\n",
            "12/01/2019 11:21:16 - INFO - root -   lr after step 29050: 4.2060209049555096e-06\n",
            "12/01/2019 11:21:16 - INFO - root -   train_loss after step 29050: 0.360757098197937\n",
            "12/01/2019 11:21:44 - INFO - root -   lr after step 29100: 4.136383924529369e-06\n",
            "12/01/2019 11:21:44 - INFO - root -   train_loss after step 29100: 0.3449969902634621\n",
            "12/01/2019 11:22:12 - INFO - root -   lr after step 29150: 4.067285531324263e-06\n",
            "12/01/2019 11:22:12 - INFO - root -   train_loss after step 29150: 0.35358839213848114\n",
            "12/01/2019 11:22:41 - INFO - root -   lr after step 29200: 3.99872716425391e-06\n",
            "12/01/2019 11:22:41 - INFO - root -   train_loss after step 29200: 0.35572985380887984\n",
            "12/01/2019 11:23:09 - INFO - root -   lr after step 29250: 3.930710250986431e-06\n",
            "12/01/2019 11:23:09 - INFO - root -   train_loss after step 29250: 0.3582567876577377\n",
            "12/01/2019 11:23:37 - INFO - root -   lr after step 29300: 3.863236207914662e-06\n",
            "12/01/2019 11:23:37 - INFO - root -   train_loss after step 29300: 0.3392381328344345\n",
            "12/01/2019 11:24:05 - INFO - root -   lr after step 29350: 3.7963064401266255e-06\n",
            "12/01/2019 11:24:05 - INFO - root -   train_loss after step 29350: 0.3473455911874771\n",
            "12/01/2019 11:24:34 - INFO - root -   lr after step 29400: 3.7299223413762975e-06\n",
            "12/01/2019 11:24:34 - INFO - root -   train_loss after step 29400: 0.3617039334774017\n",
            "12/01/2019 11:25:02 - INFO - root -   lr after step 29450: 3.664085294054569e-06\n",
            "12/01/2019 11:25:02 - INFO - root -   train_loss after step 29450: 0.35906159698963164\n",
            "12/01/2019 11:25:30 - INFO - root -   lr after step 29500: 3.5987966691604546e-06\n",
            "12/01/2019 11:25:30 - INFO - root -   train_loss after step 29500: 0.3400955164432526\n",
            "12/01/2019 11:25:58 - INFO - root -   lr after step 29550: 3.534057826272571e-06\n",
            "12/01/2019 11:25:58 - INFO - root -   train_loss after step 29550: 0.35461385488510133\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 11:26:26 - INFO - root -   lr after step 29600: 3.4698701135207854e-06\n",
            "12/01/2019 11:26:26 - INFO - root -   train_loss after step 29600: 0.3493919429183006\n",
            "12/01/2019 11:26:55 - INFO - root -   lr after step 29650: 3.4062348675581634e-06\n",
            "12/01/2019 11:26:55 - INFO - root -   train_loss after step 29650: 0.3331032133102417\n",
            "12/01/2019 11:27:23 - INFO - root -   lr after step 29700: 3.3431534135331487e-06\n",
            "12/01/2019 11:27:23 - INFO - root -   train_loss after step 29700: 0.33957488924264906\n",
            "12/01/2019 11:27:51 - INFO - root -   lr after step 29750: 3.280627065061934e-06\n",
            "12/01/2019 11:27:51 - INFO - root -   train_loss after step 29750: 0.3712329390645027\n",
            "12/01/2019 11:28:19 - INFO - root -   lr after step 29800: 3.2186571242011285e-06\n",
            "12/01/2019 11:28:19 - INFO - root -   train_loss after step 29800: 0.34505506038665773\n",
            "12/01/2019 11:28:47 - INFO - root -   lr after step 29850: 3.15724488142063e-06\n",
            "12/01/2019 11:28:47 - INFO - root -   train_loss after step 29850: 0.34652333199977875\n",
            "12/01/2019 11:29:15 - INFO - root -   lr after step 29900: 3.096391615576789e-06\n",
            "12/01/2019 11:29:16 - INFO - root -   train_loss after step 29900: 0.3606466963887215\n",
            "12/01/2019 11:29:44 - INFO - root -   lr after step 29950: 3.0360985938857177e-06\n",
            "12/01/2019 11:29:44 - INFO - root -   train_loss after step 29950: 0.33721170157194136\n",
            "12/01/2019 11:30:12 - INFO - root -   lr after step 30000: 2.976367071896947e-06\n",
            "12/01/2019 11:30:12 - INFO - root -   train_loss after step 30000: 0.35709381639957427\n",
            "12/01/2019 11:30:40 - INFO - root -   lr after step 30050: 2.9171982934672715e-06\n",
            "12/01/2019 11:30:40 - INFO - root -   train_loss after step 30050: 0.3424310287833214\n",
            "12/01/2019 11:31:08 - INFO - root -   lr after step 30100: 2.8585934907348256e-06\n",
            "12/01/2019 11:31:08 - INFO - root -   train_loss after step 30100: 0.3598847270011902\n",
            "12/01/2019 11:31:36 - INFO - root -   lr after step 30150: 2.8005538840934687e-06\n",
            "12/01/2019 11:31:36 - INFO - root -   train_loss after step 30150: 0.33506126910448075\n",
            "12/01/2019 11:32:04 - INFO - root -   lr after step 30200: 2.743080682167315e-06\n",
            "12/01/2019 11:32:04 - INFO - root -   train_loss after step 30200: 0.35476384103298186\n",
            "12/01/2019 11:32:33 - INFO - root -   lr after step 30250: 2.6861750817856234e-06\n",
            "12/01/2019 11:32:33 - INFO - root -   train_loss after step 30250: 0.35039119094610216\n",
            "12/01/2019 11:33:01 - INFO - root -   lr after step 30300: 2.6298382679578247e-06\n",
            "12/01/2019 11:33:01 - INFO - root -   train_loss after step 30300: 0.36339187681674956\n",
            "12/01/2019 11:33:29 - INFO - root -   lr after step 30350: 2.5740714138488676e-06\n",
            "12/01/2019 11:33:29 - INFO - root -   train_loss after step 30350: 0.34548385694622996\n",
            "12/01/2019 11:33:57 - INFO - root -   lr after step 30400: 2.518875680754811e-06\n",
            "12/01/2019 11:33:57 - INFO - root -   train_loss after step 30400: 0.3426385250687599\n",
            "12/01/2019 11:34:25 - INFO - root -   lr after step 30450: 2.4642522180785877e-06\n",
            "12/01/2019 11:34:25 - INFO - root -   train_loss after step 30450: 0.3511394786834717\n",
            "12/01/2019 11:34:54 - INFO - root -   lr after step 30500: 2.4102021633061166e-06\n",
            "12/01/2019 11:34:54 - INFO - root -   train_loss after step 30500: 0.3676113921403885\n",
            "12/01/2019 11:35:22 - INFO - root -   lr after step 30550: 2.356726641982595e-06\n",
            "12/01/2019 11:35:22 - INFO - root -   train_loss after step 30550: 0.3789569979906082\n",
            "12/01/2019 11:35:50 - INFO - root -   lr after step 30600: 2.30382676768905e-06\n",
            "12/01/2019 11:35:50 - INFO - root -   train_loss after step 30600: 0.3387888264656067\n",
            "12/01/2019 11:36:18 - INFO - root -   lr after step 30650: 2.2515036420191847e-06\n",
            "12/01/2019 11:36:18 - INFO - root -   train_loss after step 30650: 0.3486583912372589\n",
            "12/01/2019 11:36:46 - INFO - root -   lr after step 30700: 2.1997583545563927e-06\n",
            "12/01/2019 11:36:46 - INFO - root -   train_loss after step 30700: 0.3563654661178589\n",
            "12/01/2019 11:37:14 - INFO - root -   lr after step 30750: 2.1485919828511127e-06\n",
            "12/01/2019 11:37:14 - INFO - root -   train_loss after step 30750: 0.35845702797174456\n",
            "12/01/2019 11:37:43 - INFO - root -   lr after step 30800: 2.0980055923983465e-06\n",
            "12/01/2019 11:37:43 - INFO - root -   train_loss after step 30800: 0.3725994488596916\n",
            "12/01/2019 11:38:11 - INFO - root -   lr after step 30850: 2.048000236615515e-06\n",
            "12/01/2019 11:38:11 - INFO - root -   train_loss after step 30850: 0.35257500648498535\n",
            "12/01/2019 11:38:39 - INFO - root -   lr after step 30900: 1.9985769568204828e-06\n",
            "12/01/2019 11:38:39 - INFO - root -   train_loss after step 30900: 0.340731600522995\n",
            "12/01/2019 11:39:07 - INFO - root -   lr after step 30950: 1.9497367822098943e-06\n",
            "12/01/2019 11:39:07 - INFO - root -   train_loss after step 30950: 0.3566886100172997\n",
            "12/01/2019 11:39:36 - INFO - root -   lr after step 31000: 1.9014807298377446e-06\n",
            "12/01/2019 11:39:36 - INFO - root -   train_loss after step 31000: 0.353795682489872\n",
            "12/01/2019 11:40:04 - INFO - root -   lr after step 31050: 1.8538098045941808e-06\n",
            "12/01/2019 11:40:04 - INFO - root -   train_loss after step 31050: 0.35656840562820435\n",
            "12/01/2019 11:40:32 - INFO - root -   lr after step 31100: 1.8067249991846046e-06\n",
            "12/01/2019 11:40:32 - INFO - root -   train_loss after step 31100: 0.35631584882736206\n",
            "12/01/2019 11:41:01 - INFO - root -   lr after step 31150: 1.7602272941089704e-06\n",
            "12/01/2019 11:41:01 - INFO - root -   train_loss after step 31150: 0.3839133229851723\n",
            "12/01/2019 11:41:29 - INFO - root -   lr after step 31200: 1.714317657641382e-06\n",
            "12/01/2019 11:41:29 - INFO - root -   train_loss after step 31200: 0.38239267528057097\n",
            "12/01/2019 11:41:57 - INFO - root -   lr after step 31250: 1.6689970458099302e-06\n",
            "12/01/2019 11:41:57 - INFO - root -   train_loss after step 31250: 0.3653949111700058\n",
            "12/01/2019 11:42:25 - INFO - root -   lr after step 31300: 1.6242664023767928e-06\n",
            "12/01/2019 11:42:25 - INFO - root -   train_loss after step 31300: 0.3819098576903343\n",
            "12/01/2019 11:42:53 - INFO - root -   lr after step 31350: 1.580126658818556e-06\n",
            "12/01/2019 11:42:53 - INFO - root -   train_loss after step 31350: 0.3664112237095833\n",
            "12/01/2019 11:43:21 - INFO - root -   lr after step 31400: 1.536578734306835e-06\n",
            "12/01/2019 11:43:21 - INFO - root -   train_loss after step 31400: 0.3797431594133377\n",
            "12/01/2019 11:43:49 - INFO - root -   lr after step 31450: 1.4936235356891302e-06\n",
            "12/01/2019 11:43:49 - INFO - root -   train_loss after step 31450: 0.37395810931921003\n",
            "12/01/2019 11:44:18 - INFO - root -   lr after step 31500: 1.4512619574699527e-06\n",
            "12/01/2019 11:44:18 - INFO - root -   train_loss after step 31500: 0.34792976677417753\n",
            "12/01/2019 11:44:46 - INFO - root -   lr after step 31550: 1.4094948817921704e-06\n",
            "12/01/2019 11:44:46 - INFO - root -   train_loss after step 31550: 0.37649655163288115\n",
            "12/01/2019 11:45:14 - INFO - root -   lr after step 31600: 1.3683231784186722e-06\n",
            "12/01/2019 11:45:14 - INFO - root -   train_loss after step 31600: 0.35138659238815306\n",
            "12/01/2019 11:45:42 - INFO - root -   lr after step 31650: 1.327747704714224e-06\n",
            "12/01/2019 11:45:42 - INFO - root -   train_loss after step 31650: 0.33918940514326096\n",
            "12/01/2019 11:46:11 - INFO - root -   lr after step 31700: 1.2877693056276429e-06\n",
            "12/01/2019 11:46:11 - INFO - root -   train_loss after step 31700: 0.35553244650363924\n",
            "12/01/2019 11:46:39 - INFO - root -   lr after step 31750: 1.2483888136741761e-06\n",
            "12/01/2019 11:46:39 - INFO - root -   train_loss after step 31750: 0.3297503665089607\n",
            "12/01/2019 11:47:07 - INFO - root -   lr after step 31800: 1.2096070489181833e-06\n",
            "12/01/2019 11:47:07 - INFO - root -   train_loss after step 31800: 0.37366295158863067\n",
            "12/01/2019 11:47:35 - INFO - root -   lr after step 31850: 1.1714248189560505e-06\n",
            "12/01/2019 11:47:35 - INFO - root -   train_loss after step 31850: 0.34165135711431505\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 11:48:03 - INFO - root -   lr after step 31900: 1.1338429188993748e-06\n",
            "12/01/2019 11:48:03 - INFO - root -   train_loss after step 31900: 0.3423261530697346\n",
            "12/01/2019 11:48:32 - INFO - root -   lr after step 31950: 1.0968621313584192e-06\n",
            "12/01/2019 11:48:32 - INFO - root -   train_loss after step 31950: 0.3482692486047745\n",
            "12/01/2019 11:49:00 - INFO - root -   lr after step 32000: 1.0604832264257801e-06\n",
            "12/01/2019 11:49:00 - INFO - root -   train_loss after step 32000: 0.3530027735233307\n",
            "12/01/2019 11:49:28 - INFO - root -   lr after step 32050: 1.0247069616603933e-06\n",
            "12/01/2019 11:49:28 - INFO - root -   train_loss after step 32050: 0.35443165063858034\n",
            "12/01/2019 11:49:56 - INFO - root -   lr after step 32100: 9.895340820717358e-07\n",
            "12/01/2019 11:49:56 - INFO - root -   train_loss after step 32100: 0.3394032555818558\n",
            "12/01/2019 11:50:24 - INFO - root -   lr after step 32150: 9.549653201043063e-07\n",
            "12/01/2019 11:50:24 - INFO - root -   train_loss after step 32150: 0.33984025835990905\n",
            "12/01/2019 11:50:53 - INFO - root -   lr after step 32200: 9.210013956223929e-07\n",
            "12/01/2019 11:50:53 - INFO - root -   train_loss after step 32200: 0.3718162950873375\n",
            "12/01/2019 11:51:21 - INFO - root -   lr after step 32250: 8.876430158950621e-07\n",
            "12/01/2019 11:51:21 - INFO - root -   train_loss after step 32250: 0.3770549777150154\n",
            "12/01/2019 11:51:49 - INFO - root -   lr after step 32300: 8.548908755814511e-07\n",
            "12/01/2019 11:51:49 - INFO - root -   train_loss after step 32300: 0.3556987261772156\n",
            "12/01/2019 11:52:18 - INFO - root -   lr after step 32350: 8.22745656716275e-07\n",
            "12/01/2019 11:52:18 - INFO - root -   train_loss after step 32350: 0.359048575758934\n",
            "12/01/2019 11:52:46 - INFO - root -   lr after step 32400: 7.912080286956524e-07\n",
            "12/01/2019 11:52:46 - INFO - root -   train_loss after step 32400: 0.34169398367404935\n",
            "12/01/2019 11:53:14 - INFO - root -   lr after step 32450: 7.602786482631463e-07\n",
            "12/01/2019 11:53:14 - INFO - root -   train_loss after step 32450: 0.35789024382829665\n",
            "12/01/2019 11:53:43 - INFO - root -   lr after step 32500: 7.299581594960946e-07\n",
            "12/01/2019 11:53:43 - INFO - root -   train_loss after step 32500: 0.3402919733524323\n",
            "12/01/2019 11:54:11 - INFO - root -   lr after step 32550: 7.002471937922117e-07\n",
            "12/01/2019 11:54:11 - INFO - root -   train_loss after step 32550: 0.38153486013412474\n",
            "12/01/2019 11:54:39 - INFO - root -   lr after step 32600: 6.711463698564013e-07\n",
            "12/01/2019 11:54:39 - INFO - root -   train_loss after step 32600: 0.35807756051421163\n",
            "12/01/2019 11:55:07 - INFO - root -   lr after step 32650: 6.42656293687921e-07\n",
            "12/01/2019 11:55:07 - INFO - root -   train_loss after step 32650: 0.34272981852293016\n",
            "12/01/2019 11:55:35 - INFO - root -   lr after step 32700: 6.147775585677251e-07\n",
            "12/01/2019 11:55:35 - INFO - root -   train_loss after step 32700: 0.3658228403329849\n",
            "12/01/2019 11:56:03 - INFO - root -   lr after step 32750: 5.875107450461314e-07\n",
            "12/01/2019 11:56:03 - INFO - root -   train_loss after step 32750: 0.37261531472206116\n",
            "12/01/2019 11:56:32 - INFO - root -   lr after step 32800: 5.60856420930721e-07\n",
            "12/01/2019 11:56:32 - INFO - root -   train_loss after step 32800: 0.35336038142442705\n",
            "12/01/2019 11:57:00 - INFO - root -   lr after step 32850: 5.348151412745205e-07\n",
            "12/01/2019 11:57:00 - INFO - root -   train_loss after step 32850: 0.38744990557432174\n",
            "12/01/2019 11:57:28 - INFO - root -   lr after step 32900: 5.093874483644389e-07\n",
            "12/01/2019 11:57:28 - INFO - root -   train_loss after step 32900: 0.3535170352458954\n",
            "12/01/2019 11:57:56 - INFO - root -   lr after step 32950: 4.845738717099823e-07\n",
            "12/01/2019 11:57:56 - INFO - root -   train_loss after step 32950: 0.3694417589902878\n",
            "12/01/2019 11:58:24 - INFO - root -   lr after step 33000: 4.6037492803221004e-07\n",
            "12/01/2019 11:58:24 - INFO - root -   train_loss after step 33000: 0.3601270037889481\n",
            "12/01/2019 11:58:52 - INFO - root -   lr after step 33050: 4.3679112125299956e-07\n",
            "12/01/2019 11:58:52 - INFO - root -   train_loss after step 33050: 0.3870149424672127\n",
            "12/01/2019 11:59:21 - INFO - root -   lr after step 33100: 4.138229424845286e-07\n",
            "12/01/2019 11:59:21 - INFO - root -   train_loss after step 33100: 0.379098117351532\n",
            "12/01/2019 11:59:49 - INFO - root -   lr after step 33150: 3.914708700190728e-07\n",
            "12/01/2019 11:59:49 - INFO - root -   train_loss after step 33150: 0.326792693734169\n",
            "12/01/2019 12:00:17 - INFO - root -   lr after step 33200: 3.6973536931902083e-07\n",
            "12/01/2019 12:00:17 - INFO - root -   train_loss after step 33200: 0.3528553430736065\n",
            "12/01/2019 12:00:45 - INFO - root -   lr after step 33250: 3.486168930071887e-07\n",
            "12/01/2019 12:00:45 - INFO - root -   train_loss after step 33250: 0.34425656259059906\n",
            "12/01/2019 12:01:13 - INFO - root -   lr after step 33300: 3.2811588085741364e-07\n",
            "12/01/2019 12:01:13 - INFO - root -   train_loss after step 33300: 0.35898036569356917\n",
            "12/01/2019 12:01:41 - INFO - root -   lr after step 33350: 3.082327597853685e-07\n",
            "12/01/2019 12:01:41 - INFO - root -   train_loss after step 33350: 0.3542828345298767\n",
            "12/01/2019 12:02:09 - INFO - root -   lr after step 33400: 2.8896794383969195e-07\n",
            "12/01/2019 12:02:09 - INFO - root -   train_loss after step 33400: 0.34810379773378375\n",
            "12/01/2019 12:02:38 - INFO - root -   lr after step 33450: 2.703218341933522e-07\n",
            "12/01/2019 12:02:38 - INFO - root -   train_loss after step 33450: 0.35508797913789747\n",
            "12/01/2019 12:03:06 - INFO - root -   lr after step 33500: 2.5229481913529695e-07\n",
            "12/01/2019 12:03:06 - INFO - root -   train_loss after step 33500: 0.3507480914890766\n",
            "12/01/2019 12:03:34 - INFO - root -   lr after step 33550: 2.3488727406237642e-07\n",
            "12/01/2019 12:03:34 - INFO - root -   train_loss after step 33550: 0.3434665766358376\n",
            "12/01/2019 12:04:02 - INFO - root -   lr after step 33600: 2.1809956147151976e-07\n",
            "12/01/2019 12:04:02 - INFO - root -   train_loss after step 33600: 0.3497272226214409\n",
            "12/01/2019 12:04:30 - INFO - root -   lr after step 33650: 2.0193203095218104e-07\n",
            "12/01/2019 12:04:30 - INFO - root -   train_loss after step 33650: 0.3699278798699379\n",
            "12/01/2019 12:04:58 - INFO - root -   lr after step 33700: 1.8638501917906504e-07\n",
            "12/01/2019 12:04:58 - INFO - root -   train_loss after step 33700: 0.3424438238143921\n",
            "12/01/2019 12:05:26 - INFO - root -   lr after step 33750: 1.7145884990511618e-07\n",
            "12/01/2019 12:05:26 - INFO - root -   train_loss after step 33750: 0.34651975363492965\n",
            "12/01/2019 12:05:55 - INFO - root -   lr after step 33800: 1.5715383395478066e-07\n",
            "12/01/2019 12:05:55 - INFO - root -   train_loss after step 33800: 0.36110114470124244\n",
            "12/01/2019 12:06:23 - INFO - root -   lr after step 33850: 1.4347026921751827e-07\n",
            "12/01/2019 12:06:23 - INFO - root -   train_loss after step 33850: 0.3503993362188339\n",
            "12/01/2019 12:06:51 - INFO - root -   lr after step 33900: 1.304084406416206e-07\n",
            "12/01/2019 12:06:51 - INFO - root -   train_loss after step 33900: 0.391081357896328\n",
            "12/01/2019 12:07:19 - INFO - root -   lr after step 33950: 1.1796862022825927e-07\n",
            "12/01/2019 12:07:19 - INFO - root -   train_loss after step 33950: 0.34274148285388945\n",
            "12/01/2019 12:07:47 - INFO - root -   lr after step 34000: 1.0615106702583699e-07\n",
            "12/01/2019 12:07:47 - INFO - root -   train_loss after step 34000: 0.3550391834974289\n",
            "12/01/2019 12:08:15 - INFO - root -   lr after step 34050: 9.495602712457529e-08\n",
            "12/01/2019 12:08:15 - INFO - root -   train_loss after step 34050: 0.3306776815652847\n",
            "12/01/2019 12:08:43 - INFO - root -   lr after step 34100: 8.438373365140862e-08\n",
            "12/01/2019 12:08:43 - INFO - root -   train_loss after step 34100: 0.3535468554496765\n",
            "12/01/2019 12:09:12 - INFO - root -   lr after step 34150: 7.443440676511481e-08\n",
            "12/01/2019 12:09:12 - INFO - root -   train_loss after step 34150: 0.34347035229206085\n",
            "12/01/2019 12:09:40 - INFO - root -   lr after step 34200: 6.51082536517389e-08\n",
            "12/01/2019 12:09:40 - INFO - root -   train_loss after step 34200: 0.3667341747879982\n",
            "12/01/2019 12:10:08 - INFO - root -   lr after step 34250: 5.6405468520276394e-08\n",
            "12/01/2019 12:10:08 - INFO - root -   train_loss after step 34250: 0.3552419701218605\n",
            "12/01/2019 12:10:36 - INFO - root -   lr after step 34300: 4.832623259862667e-08\n",
            "12/01/2019 12:10:36 - INFO - root -   train_loss after step 34300: 0.36124309837818147\n",
            "12/01/2019 12:11:04 - INFO - root -   lr after step 34350: 4.08707141298259e-08\n",
            "12/01/2019 12:11:04 - INFO - root -   train_loss after step 34350: 0.3685599029064178\n",
            "12/01/2019 12:11:32 - INFO - root -   lr after step 34400: 3.4039068368536544e-08\n",
            "12/01/2019 12:11:32 - INFO - root -   train_loss after step 34400: 0.35472328811883924\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 12:12:01 - INFO - root -   lr after step 34450: 2.7831437577813256e-08\n",
            "12/01/2019 12:12:01 - INFO - root -   train_loss after step 34450: 0.38236106097698214\n",
            "12/01/2019 12:12:29 - INFO - root -   lr after step 34500: 2.2247951026148628e-08\n",
            "12/01/2019 12:12:29 - INFO - root -   train_loss after step 34500: 0.34557484537363053\n",
            "12/01/2019 12:12:57 - INFO - root -   lr after step 34550: 1.7288724984775296e-08\n",
            "12/01/2019 12:12:57 - INFO - root -   train_loss after step 34550: 0.34328680276870727\n",
            "12/01/2019 12:13:25 - INFO - root -   lr after step 34600: 1.2953862725241238e-08\n",
            "12/01/2019 12:13:25 - INFO - root -   train_loss after step 34600: 0.3669495138525963\n",
            "12/01/2019 12:13:53 - INFO - root -   lr after step 34650: 9.243454517274818e-09\n",
            "12/01/2019 12:13:53 - INFO - root -   train_loss after step 34650: 0.3542681756615639\n",
            "12/01/2019 12:14:21 - INFO - root -   lr after step 34700: 6.157577626879629e-09\n",
            "12/01/2019 12:14:21 - INFO - root -   train_loss after step 34700: 0.3147313317656517\n",
            "12/01/2019 12:14:49 - INFO - root -   lr after step 34750: 3.696296314755765e-09\n",
            "12/01/2019 12:14:49 - INFO - root -   train_loss after step 34750: 0.3420757392048836\n",
            "12/01/2019 12:15:18 - INFO - root -   lr after step 34800: 1.859661834927584e-09\n",
            "12/01/2019 12:15:18 - INFO - root -   train_loss after step 34800: 0.36994742006063464\n",
            "12/01/2019 12:15:46 - INFO - root -   lr after step 34850: 6.477124337145313e-10\n",
            "12/01/2019 12:15:46 - INFO - root -   train_loss after step 34850: 0.36490960001945494\n",
            "12/01/2019 12:16:14 - INFO - root -   lr after step 34900: 6.047334889514033e-11\n",
            "12/01/2019 12:16:14 - INFO - root -   train_loss after step 34900: 0.35214207738637926\n",
            "12/01/2019 12:16:26 - INFO - root -   lr after epoch 1: 0.0\n",
            "12/01/2019 12:16:26 - INFO - root -   train_loss after epoch 1: 0.3752534805951345\n",
            "12/01/2019 12:16:26 - INFO - root -   \n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34922, 0.3752534805951345)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAwTU9GYIdUt",
        "colab_type": "code",
        "outputId": "27bbd4d0-539a-4444-a37d-5df03c8976d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Save the model into a file\n",
        "learner.save_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/01/2019 12:16:28 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/ML-data/tweetsense/output/model_out/config.json\n",
            "12/01/2019 12:16:29 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/ML-data/tweetsense/output/model_out/pytorch_model.bin\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAsLYicHLtGb",
        "colab_type": "text"
      },
      "source": [
        "### Step 7: Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2_jDTVSG1oM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fast_bert.prediction import BertClassificationPredictor\n",
        "\n",
        "MODEL_PATH = \"/content/drive/My Drive/ML-data/tweetsense/output/model_out\"\n",
        "\n",
        "predictor = BertClassificationPredictor(\n",
        "\t\t\t\tmodel_path=MODEL_PATH,\n",
        "\t\t\t\tlabel_path=LABEL_PATH, # location for labels.csv file\n",
        "\t\t\t\tmulti_label=False,\n",
        "\t\t\t\tmodel_type='bert',\n",
        "\t\t\t\tdo_lower_case=False)\n",
        "\n",
        "def predict_sentiment(tweet):\n",
        "    prediction = predictor.predict(tweet)\n",
        "    \n",
        "    # if prediction[0][1] < 0.65 and prediction[1][1] < 0.65:\n",
        "    #     # print(\"Neutral\")\n",
        "    #     return 2\n",
        "    # else:\n",
        "    result = max(prediction, key=lambda x: x[1])\n",
        "    # if result[0] == \"0\":\n",
        "        # print(\"Negative\")\n",
        "    # elif result[0] == \"4\":\n",
        "        # print(\"Positive\")\n",
        "\n",
        "    return result[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12j_0NgnOHBG",
        "colab_type": "code",
        "outputId": "6cc4afc3-0e1f-40f6-c923-fe9016c1fa5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_sentiment(\"just get me result for this text\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-_aINHwKMKh",
        "colab_type": "code",
        "outputId": "a8c9c706-b025-47d5-adcc-b2a917edb00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predict_sentiment(\"I hate you\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiE6DL1XK-U7",
        "colab_type": "code",
        "outputId": "daf3eb0f-49f2-49eb-d57b-8a194331dc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predict_sentiment(\"fuck this shit\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZssnhu9LPWg",
        "colab_type": "code",
        "outputId": "273d6a99-8bbb-406c-fd17-b43c3b1cd8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predict_sentiment(\"I love you\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5lVx5ykOzVE",
        "colab_type": "code",
        "outputId": "eeb0d12c-eb25-406c-e29c-1b5334a17fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predict_sentiment(\"This is stupid\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o__PGly5O5k6",
        "colab_type": "text"
      },
      "source": [
        "### Step 8: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJxFvNiFO1gR",
        "colab_type": "code",
        "outputId": "70d2ac6a-1cae-4187-f308-b58f3e7f1b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "dir = \"/content/drive/My Drive/ML-data/tweetsense/sentiment140/\"\n",
        "\n",
        "col_names = ['label', 'id', 'date', 'flag', 'user', 'text']\n",
        "df = pd.read_csv(dir + '/test.csv', encoding='latin-1', names=col_names)\n",
        "df = df[['text', 'label']]\n",
        "print(df.head(2))\n",
        "print(\"\\nThere are {} tweets in total.\\n\".format(len(df)))\n",
        "print(\"There are {} positve tweets\".format(np.sum(df[\"label\"] == 4)))\n",
        "print(\"There are {} neutral tweets\".format(np.sum(df[\"label\"] == 2)))\n",
        "print(\"There are {} negative tweets\".format(np.sum(df[\"label\"] == 0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  label\n",
            "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...      4\n",
            "1  Reading my kindle2...  Love it... Lee childs i...      4\n",
            "\n",
            "There are 498 tweets in total.\n",
            "\n",
            "There are 182 positve tweets\n",
            "There are 139 neutral tweets\n",
            "There are 177 negative tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4G_6Qi5PcwH",
        "colab_type": "code",
        "outputId": "7b02bde2-248d-4b54-f1a1-669aa2efb0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "df = df[df['label'] != 2]\n",
        "y_true = df['label'].values.tolist()\n",
        "X_test = df['text'].values.tolist()\n",
        "print(len(X_test))\n",
        "print(len(y_true))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "359\n",
            "359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsI_GH9wPi71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = []\n",
        "\n",
        "for x in X_test:\n",
        "    x = preprocess(x)\n",
        "\n",
        "    if x == \"\":\n",
        "        print(\"Empty string. Assume positive\")\n",
        "        y_pred.append(4)\n",
        "\n",
        "    predicted_label = predict_sentiment(x)\n",
        "    y_pred.append(int(predicted_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsm_cETcQKIh",
        "colab_type": "code",
        "outputId": "4627b798-b6b0-42ac-b613-bc9631e5887f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(y_true)\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 0, 4, 4, 0, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 0, 4, 4, 0, 0, 4, 4, 4, 0, 4, 4, 4, 0, 0, 4, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 0, 4, 4, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 4, 0, 4, 4, 4, 4, 4, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 4, 0, 0, 4, 4, 4, 0, 4, 4, 4, 4, 0, 4, 0, 4, 0, 0, 0, 0, 4, 0, 4, 0, 4, 4, 4, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 4, 4, 0, 4, 4, 0, 4, 0, 0]\n",
            "[4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 0, 0, 0, 0, 4, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 0, 4, 4, 0, 0, 4, 4, 4, 0, 4, 4, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 4, 4, 4, 4, 0, 4, 0, 4, 4, 0, 4, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 0, 4, 4, 4, 4, 0, 0, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 0, 0, 4, 4, 4, 0, 4, 4, 0, 0, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 4, 0, 0, 0, 0, 4, 0, 4, 0, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 0, 0, 0, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 4, 4, 4, 0, 0, 4, 4, 4, 0, 4, 0, 4, 4, 0, 4, 4, 0, 0, 4, 0, 4, 0, 4, 0, 4, 4, 4, 0, 0, 4, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf6tmvAeQSTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFCEypqVQbkm",
        "colab_type": "code",
        "outputId": "91b43471-c3e8-4010-cc32-1c15f537601a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(y_true, y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8328690807799443"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3nDN9z5Qdkz",
        "colab_type": "code",
        "outputId": "36a7bc62-e354-46d5-93b4-ef29cbe6c83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1_score(y_true, y_pred, average='weighted') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8328223671689822"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4dljSL1T1U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}